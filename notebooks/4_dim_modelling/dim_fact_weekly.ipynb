{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3720f3-bde0-4cb9-b002-7063dccba12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ad945f-c4d7-4b3e-91fa-c7f10c0d97ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff544fcc-f81c-47a9-8016-9f93634d43e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None,\n",
    "    merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by,\n",
    "            merge_schema=merge_schema\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1424de-aa25-48f2-a877-86eb256cb6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"prod\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "    \n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "feature_schema = f\"fpl_feature_{ENV}\"\n",
    "gold_schema = f\"fpl_gold_{ENV}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0676cacd-37ee-45cd-9c45-6ee68586a14e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim players"
    }
   },
   "outputs": [],
   "source": [
    "position_df = spark.table(f\"{silver_schema}.positions\").select(\"position_key\", \"position_name\", \"position_name_short\")\n",
    "team_df = spark.table(f\"{silver_schema}.teams\").select(\"team_key\", \"team_season_key\", \"team_name_short\").distinct()\n",
    "\n",
    "player_raw_df = spark.table(f\"{silver_schema}.players\"\n",
    "        ).withColumnRenamed(\n",
    "            \"team_season_key\", \"_team_season_key\"\n",
    "        ).withColumn(\n",
    "            \"player_name\", \n",
    "            F.concat_ws(\" \", F.col(\"first_name\"), F.col(\"second_name\"))\n",
    "        ).drop(\"team_key\")\n",
    "        \n",
    "        \n",
    "player_df = player_raw_df.join(\n",
    "            team_df, \n",
    "            on= player_raw_df[\"_team_season_key\"] == team_df[\"team_season_key\"], \n",
    "            how=\"inner\"\n",
    "        ).join(\n",
    "            position_df, \n",
    "            on=\"position_key\", \n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "window_spec = Window.partitionBy(\"player_name\", \"season_key\").orderBy(\"player_surrogate_key\")\n",
    "player_df = player_df.withColumn(\n",
    "    \"is_duplicate\",\n",
    "    F.when(F.count(\"player_surrogate_key\").over(window_spec) > 1, F.lit(True)).otherwise(F.lit(False))\n",
    ")\n",
    "\n",
    "player_df = player_df.withColumn(\n",
    "    \"player_name\",\n",
    "    F.when(\n",
    "        F.col(\"is_duplicate\"),\n",
    "        F.concat_ws(\" \", F.col(\"player_name\"), F.col(\"team_name_short\"))\n",
    "    ).otherwise(F.col(\"player_name\"))\n",
    ")\n",
    "\n",
    "player_df = player_df.select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"player_key\",\n",
    "    \"player_season_spell_key\",\n",
    "    \"season_key\",\n",
    "    \"player_name\",\n",
    "    \"player_season_key\",\n",
    "    \"team_key\",\n",
    "    \"team_season_key\",\n",
    "    \"team_name_short\",\n",
    "    \"position_name\",\n",
    "    \"initial_value\",\n",
    "    \"current_value\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\"\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df=player_df,\n",
    "    table_name=f\"{gold_schema}.dim_player\",\n",
    "    merge_condition=\"target.player_surrogate_key = source.player_surrogate_key\",\n",
    "    spark=spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93adac7c-be6a-4868-838e-75f1ea06908d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim gameweek"
    }
   },
   "outputs": [],
   "source": [
    "gameweek_df = spark.table(f\"{silver_schema}.gameweeks\")\n",
    "\n",
    "merge_to_table(\n",
    "    df = gameweek_df,\n",
    "    table_name = f\"{gold_schema}.dim_gameweek\",\n",
    "    merge_condition = \"target.gameweek_key = source.gameweek_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7a5cfc-9b90-43b2-acee-6dff6567a57c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim fixtures"
    }
   },
   "outputs": [],
   "source": [
    "team_df = spark.table(f\"{silver_schema}.teams\").select(\"team_key\", \"team_season_key\", \"team_name_short\").distinct()\n",
    "\n",
    "fixture_df = spark.table(f\"{silver_schema}.fixtures\").select(\n",
    "    \"fixture_key\",\n",
    "    \"season_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"home_team_key\",\n",
    "    \"home_team_season_key\",\n",
    "    \"away_team_key\",\n",
    "    \"away_team_season_key\",\n",
    "    \"home_team_score\",\n",
    "    \"away_team_score\",\n",
    "    \"kickoff_time\",\n",
    "    \"gameweek\"\n",
    ")\n",
    "\n",
    "fixture_df = fixture_df.join(\n",
    "    team_df.withColumnRenamed(\"team_season_key\", \"home_team_season_key\"\n",
    "        ).withColumnRenamed(\"team_name_short\", \"home_team_name\"),\n",
    "    on=\"home_team_season_key\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    team_df.withColumnRenamed(\"team_key\", \"away_team_season_key\"\n",
    "        ).withColumnRenamed(\"team_name_short\", \"away_team_name\"),\n",
    "    on=\"away_team_season_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = fixture_df,\n",
    "    table_name = f\"{gold_schema}.dim_fixture\",\n",
    "    merge_condition = \"target.fixture_key = source.fixture_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca95a3aa-d494-4614-aa50-54a6c403189e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact player stats"
    }
   },
   "outputs": [],
   "source": [
    "players = spark.table(f\"{silver_schema}.players\").select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"player_key\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\"\n",
    ")\n",
    "\n",
    "player_stats = spark.table(f\"{silver_schema}.gameweek_stats\").select(\n",
    "    \"player_key\",\n",
    "    \"season_key\",\n",
    "    \"fixture_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"team_key\",\n",
    "    \"team_season_key\",\n",
    "    \"position_key\",\n",
    "    \"opponent_team_key\",\n",
    "    \"opponent_team_season_key\",\n",
    "    \"minutes\",\n",
    "    \"assists\",\n",
    "    \"goals_scored\",\n",
    "    \"goals_conceded\",\n",
    "    \"clean_sheets\",\n",
    "    \"own_goals\",\n",
    "    \"bps\",\n",
    "    \"clearances_blocks_interceptions\",\n",
    "    \"recoveries\",\n",
    "    \"creativity\",\n",
    "    \"defensive_contribution\",\n",
    "    \"tackles\",\n",
    "    \"saves\",\n",
    "    \"yellow_cards\",\n",
    "    \"red_cards\",\n",
    "    \"expected_assists\",\n",
    "    \"expected_goals\",\n",
    "    \"expected_goal_involvements\",\n",
    "    \"expected_goals_conceded\",\n",
    "    \"penalties_saved\",\n",
    "    \"penalties_missed\",\n",
    "    \"ict_index\",\n",
    "    \"influence\",\n",
    "    \"value\",\n",
    "    \"was_home\"\n",
    ")\n",
    "\n",
    "player_stats = player_stats.join(\n",
    "    players,\n",
    "    (player_stats.player_key == players.player_key) &\n",
    "    (player_stats.gameweek_key >= players.effective_from) &\n",
    "    (player_stats.gameweek_key <= players.effective_to),\n",
    "    \"left\"\n",
    ").drop(\n",
    "    \"effective_from\",\n",
    "    \"effective_to\",\n",
    "    players.player_key\n",
    ").withColumn(\n",
    "    \"player_gw_stat_key\",\n",
    "    F.concat(F.col(\"player_surrogate_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = player_stats,\n",
    "    table_name = f\"{gold_schema}.fact_player_gameweek_stats\",\n",
    "    merge_condition = \"target.player_gw_stat_key = source.player_gw_stat_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ca9f0a-ddbb-4485-8f91-4ea0c554c71e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact player fpl points"
    }
   },
   "outputs": [],
   "source": [
    "players = spark.table(f\"{silver_schema}.players\").select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"player_key\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\"\n",
    ")\n",
    "\n",
    "player_points = spark.table(f\"{silver_schema}.gameweek_stats\").select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"season_key\",\n",
    "    \"fixture_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"team_key\",\n",
    "    \"team_season_key\",\n",
    "    \"position_key\",\n",
    "    \"opponent_team_key\",\n",
    "    \"opponent_team_season_key\",\n",
    "    \"minutes_points\",\n",
    "    \"assist_points\",\n",
    "    \"goal_points\",\n",
    "    \"clean_sheet_points\",\n",
    "    \"defensive_contribution_points\",\n",
    "    \"goals_conceded_points\",\n",
    "    \"own_goal_points\",\n",
    "    \"penalty_miss_points\",\n",
    "    \"penalty_saves_points\",\n",
    "    \"tackles\",\n",
    "    \"saves_points\",\n",
    "    \"yellow_card_points\",\n",
    "    \"red_card_points\",\n",
    "    F.col(\"bonus\").alias(\"bonus_points\")\n",
    ")\n",
    "\n",
    "player_points = player_points.join(\n",
    "    players,\n",
    "    (player_points.player_surrogate_key == players.player_surrogate_key),\n",
    "    \"left\"\n",
    ").drop(\n",
    "    \"effective_from\",\n",
    "    \"effective_to\",\n",
    "    players.player_surrogate_key\n",
    ").withColumn(\n",
    "    \"player_gw_stat_key\",\n",
    "    F.concat(F.col(\"player_surrogate_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ").withColumn(\n",
    "    \"total_points\",\n",
    "    F.coalesce(F.col(\"minutes_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"assist_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"goal_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"clean_sheet_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"defensive_contribution_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"goals_conceded_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"own_goal_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"penalty_miss_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"penalty_saves_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"saves_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"yellow_card_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"red_card_points\"), F.lit(0)) +\n",
    "    F.coalesce(F.col(\"bonus_points\"), F.lit(0))\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = player_points,\n",
    "    table_name = f\"{gold_schema}.fact_player_fpl_points\",\n",
    "    merge_condition = \"target.player_gw_stat_key = source.player_gw_stat_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d829f4-4bbe-4636-8481-2c08a839b56f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact team gameweek stats"
    }
   },
   "outputs": [],
   "source": [
    "player_stats = spark.table(f\"{silver_schema}.gameweek_stats\").select(\n",
    "    \"team_key\",\n",
    "    \"team_season_key\",\n",
    "    \"fixture_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"season_key\",\n",
    "    \"minutes\",\n",
    "    \"assists\",\n",
    "    \"goals_scored\",\n",
    "    \"goals_conceded\",\n",
    "    \"clean_sheets\",\n",
    "    \"own_goals\",\n",
    "    \"bps\",\n",
    "    \"clearances_blocks_interceptions\",\n",
    "    \"recoveries\",\n",
    "    \"creativity\",\n",
    "    \"defensive_contribution\",\n",
    "    \"tackles\",\n",
    "    \"saves\",\n",
    "    \"yellow_cards\",\n",
    "    \"red_cards\",\n",
    "    \"expected_assists\",\n",
    "    \"expected_goals\",\n",
    "    \"expected_goal_involvements\",\n",
    "    \"expected_goals_conceded\",\n",
    "    \"penalties_saved\",\n",
    "    \"penalties_missed\",\n",
    "    \"ict_index\",\n",
    "    \"influence\",\n",
    "    \"value\",\n",
    "    \"opponent_team_key\",\n",
    "    \"opponent_team_season_key\"\n",
    ")\n",
    "\n",
    "agg_exprs = [\n",
    "    F.sum(\"minutes\").alias(\"minutes\"),\n",
    "    F.sum(\"assists\").alias(\"assists\"),\n",
    "    F.sum(\"goals_scored\").alias(\"goals_scored_raw\"),\n",
    "    F.max(\"goals_conceded\").alias(\"goals_conceded\"),\n",
    "    F.max(\"clean_sheets\").alias(\"clean_sheets\"),\n",
    "    F.sum(\"own_goals\").alias(\"own_goals\"),\n",
    "    F.sum(\"bps\").alias(\"bps\"),\n",
    "    F.sum(\"clearances_blocks_interceptions\").alias(\"clearances_blocks_interceptions\"),\n",
    "    F.sum(\"recoveries\").alias(\"recoveries\"),\n",
    "    F.round(F.sum(\"creativity\"), 3).alias(\"creativity\"),\n",
    "    F.sum(\"defensive_contribution\").alias(\"defensive_contribution\"),\n",
    "    F.sum(\"tackles\").alias(\"tackles\"),\n",
    "    F.sum(\"saves\").alias(\"saves\"),\n",
    "    F.sum(\"yellow_cards\").alias(\"yellow_cards\"),\n",
    "    F.sum(\"red_cards\").alias(\"red_cards\"),\n",
    "    F.round(F.sum(\"expected_assists\"), 3).alias(\"expected_assists\"),\n",
    "    F.round(F.sum(\"expected_goals\"), 3).alias(\"expected_goals\"),\n",
    "    F.round(F.sum(\"expected_goal_involvements\"), 3).alias(\"expected_goal_involvements\"),\n",
    "    F.round(F.sum(\"expected_goals_conceded\"), 3).alias(\"expected_goals_conceded\"),\n",
    "    F.sum(\"penalties_saved\").alias(\"penalties_saved\"),\n",
    "    F.sum(\"penalties_missed\").alias(\"penalties_missed\")\n",
    "]\n",
    "\n",
    "team_gw_stats = player_stats.groupBy(\n",
    "    \"team_season_key\", \"fixture_key\", \"gameweek_key\", \"season_key\"\n",
    ").agg(*agg_exprs)\n",
    "\n",
    "# Calculate sum of own_goals from players with opponent_team_key for the same fixture\n",
    "own_goals_against_df = player_stats.groupBy(\n",
    "    \"opponent_team_season_key\", \"fixture_key\", \"gameweek_key\", \"season_key\"\n",
    ").agg(\n",
    "    F.sum(\"own_goals\").alias(\"own_goals_against\")\n",
    ").withColumnRenamed(\"opponent_team_season_key\", \"team_season_key\")\n",
    "\n",
    "team_gw_stats = team_gw_stats.join(\n",
    "    own_goals_against_df,\n",
    "    on=[\"team_season_key\", \"fixture_key\", \"gameweek_key\", \"season_key\"],\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"goals_scored\",\n",
    "    F.col(\"goals_scored_raw\") + F.coalesce(F.col(\"own_goals_against\"), F.lit(0))\n",
    ").drop(\"goals_scored_raw\", \"own_goals_against\")\n",
    "\n",
    "team_gw_stats = team_gw_stats.withColumn(\n",
    "    \"team_gw_stat_key\",\n",
    "    F.concat(F.col(\"team_season_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = team_gw_stats,\n",
    "    table_name = f\"{gold_schema}.fact_team_gameweek_stats\",\n",
    "    merge_condition = \"target.team_gw_stat_key = source.team_gw_stat_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d664fbf7-8de0-4497-a848-cb95ed45ae2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact fixture"
    }
   },
   "outputs": [],
   "source": [
    "#fact table as well as dim to expand for future reference - can add in match stats, scorers, attendance etc.\n",
    "team_df = spark.table(f\"{silver_schema}.teams\").select(\"team_season_key\", \"team_name_short\").distinct()\n",
    "\n",
    "fixture_df = spark.table(f\"{silver_schema}.fixtures\").select(\n",
    "    \"fixture_key\",\n",
    "    \"season_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"home_team_key\",\n",
    "    \"home_team_season_key\",\n",
    "    \"away_team_key\",\n",
    "    \"away_team_season_key\",\n",
    "    \"home_team_score\",\n",
    "    \"away_team_score\",\n",
    "    \"kickoff_time\",\n",
    "    \"gameweek\"\n",
    ")\n",
    "\n",
    "fixture_df = fixture_df.join(\n",
    "    team_df.withColumnRenamed(\"team_season_key\", \"home_team_season_key\"\n",
    "        ).withColumnRenamed(\"team_name_short\", \"home_team_name\"),\n",
    "    on=\"home_team_season_key\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    team_df.withColumnRenamed(\"team_season_key\", \"away_team_season_key\"\n",
    "        ).withColumnRenamed(\"team_name_short\", \"away_team_name\"),\n",
    "    on=\"away_team_season_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = fixture_df,\n",
    "    table_name = f\"{gold_schema}.fact_fixture\",\n",
    "    merge_condition = \"target.fixture_key = source.fixture_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00e7005-f1bb-4a55-b2b5-76bde2f77711",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact predicted fpl vs actual"
    }
   },
   "outputs": [],
   "source": [
    "players = spark.table(f\"{silver_schema}.players\").select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"team_season_key\",\n",
    "    \"position_key\",\n",
    "    \"effective_from\",\n",
    "    \"effective_to\"\n",
    ")\n",
    "\n",
    "player_predicted_points = spark.table(f\"{feature_schema}.player_points_inference\").select(\n",
    "    \"player_surrogate_key\",\n",
    "    \"fixture_key\",\n",
    "    \"predicted_total_points\"\n",
    ")\n",
    "\n",
    "fixture_gw = spark.table(f\"{silver_schema}.fixtures\").select(\n",
    "    \"fixture_key\",\n",
    "    \"gameweek_key\",\n",
    "    \"season_key\",\n",
    "    \"home_team_season_key\",\n",
    "    \"away_team_season_key\"\n",
    ")\n",
    "\n",
    "player_predicted_points = player_predicted_points.join(\n",
    "    fixture_gw,\n",
    "    on=\"fixture_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "player_points = player_predicted_points.join(\n",
    "    players,\n",
    "    (player_predicted_points.player_surrogate_key == players.player_surrogate_key) &\n",
    "    (player_predicted_points.gameweek_key >= (players.effective_from)) &\n",
    "    (player_predicted_points.gameweek_key <= (players.effective_to + 6)),\n",
    "    \"left\"\n",
    ").drop(\n",
    "    \"effective_from\",\n",
    "    \"effective_to\",\n",
    "    players.player_surrogate_key\n",
    ").withColumn(\n",
    "    \"player_gw_stat_key\",\n",
    "    F.concat(F.col(\"player_surrogate_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ").withColumn(\n",
    "    \"opponent_team_season_key\",\n",
    "    F.when(\n",
    "        F.col(\"team_season_key\") == F.col(\"home_team_season_key\"),\n",
    "        F.col(\"away_team_season_key\")\n",
    "    ).otherwise(F.col(\"home_team_season_key\"))\n",
    ").withColumn(\n",
    "    \"is_home\",\n",
    "    F.col(\"team_season_key\") == F.col(\"home_team_season_key\")\n",
    ").drop(\n",
    "    \"home_team_season_key\",\n",
    "    \"away_team_season_key\"\n",
    ")\n",
    "\n",
    "fpl_points = spark.table(f\"{gold_schema}.fact_player_fpl_points\").select(\n",
    "    F.col(\"player_gw_stat_key\").alias(\"_player_gw_stat_key\"),\n",
    "    \"total_points\"\n",
    ")\n",
    "\n",
    "player_predicted_actual_points = player_points.join(\n",
    "    fpl_points,\n",
    "    player_points.player_gw_stat_key == fpl_points._player_gw_stat_key,\n",
    "    \"left\"\n",
    ").drop(\"_player_gw_stat_key\"\n",
    ").withColumn(\n",
    "    \"prediction_variance\",\n",
    "    F.col(\"total_points\") - F.col(\"predicted_total_points\")\n",
    ").withColumn(\n",
    "    \"prediction_variance_abs\",\n",
    "    F.abs(F.col(\"total_points\") - F.col(\"predicted_total_points\"))\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = player_predicted_actual_points,\n",
    "    table_name = f\"{gold_schema}.fact_fpl_point_prediction_performance\",\n",
    "    merge_condition = \"target.player_gw_stat_key = source.player_gw_stat_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53df2369-77c1-4d63-a95e-80c8375b85ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact team features"
    }
   },
   "outputs": [],
   "source": [
    "team_features_df = spark.table(f\"{feature_schema}.team_features\").withColumn(\n",
    "    \"team_season_gw_key\",\n",
    "    F.concat(F.col(\"team_season_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = team_features_df,\n",
    "    table_name = f\"{gold_schema}.fact_team_features\",\n",
    "    merge_condition = \"target.team_season_gw_key = source.team_season_gw_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ecb121b-fab6-4648-aee1-dbcf4e0e08ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact player features"
    }
   },
   "outputs": [],
   "source": [
    "player_features_df = spark.table(f\"{feature_schema}.player_features\").withColumn(\n",
    "    \"player_gw_stat_key\",\n",
    "    F.concat(F.col(\"player_surrogate_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    ")\n",
    "\n",
    "merge_to_table(\n",
    "    df = player_features_df,\n",
    "    table_name = f\"{gold_schema}.fact_player_features\",\n",
    "    merge_condition = \"target.player_gw_stat_key = source.player_gw_stat_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc9699c-4eb5-4988-8be5-b9bc7a32e19e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact_fpl_events"
    }
   },
   "outputs": [],
   "source": [
    "fpl_events_df = spark.table(f\"{silver_schema}.fpl_events\")\n",
    "\n",
    "merge_to_table(\n",
    "    df = fpl_events_df,\n",
    "    table_name = f\"{gold_schema}.fact_fpl_events\",\n",
    "    merge_condition = \"target.gameweek_key = source.gameweek_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c790c2-6952-4a29-a1ac-56cbc13ef754",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769013791387}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "fact totw"
    }
   },
   "outputs": [],
   "source": [
    "# fpl_prediction_df = spark.table(f\"{gold_schema}.fact_fpl_point_prediction_performance\").select(\n",
    "#     \"player_surrogate_key\",\n",
    "#     \"position_key\",\n",
    "#     \"fixture_key\",\n",
    "#     \"gameweek_key\",\n",
    "#     \"season_key\",\n",
    "#     \"team_season_key\",\n",
    "#     \"opponent_team_season_key\",\n",
    "#     \"is_home\",\n",
    "#     \"predicted_total_points\"\n",
    "# )\n",
    "\n",
    "# fpl_player_price = spark.table(f\"{silver_schema}.players\").select(\n",
    "#     \"player_surrogate_key\",\n",
    "#     \"current_value\"\n",
    "# ) #no history of previous value\n",
    "\n",
    "# fpl_df = fpl_prediction_df.join(\n",
    "#     fpl_player_price,\n",
    "#     on=\"player_surrogate_key\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# # Position constraints\n",
    "# position_limits = {1: 2, 2: 5, 3: 5, 4: 3}\n",
    "# max_team_players = 3\n",
    "# max_total_value = 1000\n",
    "\n",
    "# # Add totw_gameweek_key\n",
    "# fpl_df = fpl_df.withColumn(\"totw_gameweek_key\", F.col(\"gameweek_key\"))\n",
    "\n",
    "# # Window for ranking by predicted points within each gameweek, position, and team\n",
    "# window_gw_pos = Window.partitionBy(\"gameweek_key\", \"position_key\").orderBy(F.col(\"predicted_total_points\").desc())\n",
    "# window_gw_team = Window.partitionBy(\"gameweek_key\", \"team_season_key\").orderBy(F.col(\"predicted_total_points\").desc())\n",
    "\n",
    "# # Rank within position and team constraints\n",
    "# fpl_df = fpl_df.withColumn(\"pos_rank\", F.row_number().over(window_gw_pos))\n",
    "# fpl_df = fpl_df.withColumn(\"team_rank\", F.row_number().over(window_gw_team))\n",
    "\n",
    "# # Filter by position and team constraints\n",
    "# fpl_df_filtered = fpl_df.filter(\n",
    "#     (F.col(\"pos_rank\") <= F.create_map(\n",
    "#         *[item for kv in position_limits.items() for item in (F.lit(kv[0]), F.lit(kv[1]))]\n",
    "#     )[F.col(\"position_key\")]) &\n",
    "#     (F.col(\"team_rank\") <= max_team_players)\n",
    "# )\n",
    "\n",
    "# display(fpl_df_filtered)\n",
    "\n",
    "# def select_totw(pdf):\n",
    "#     import pandas as pd\n",
    "#     from itertools import combinations, product\n",
    "\n",
    "#     pos_groups = {k: pdf[pdf.position_key == k] for k in position_limits}\n",
    "#     pos_combos = [list(combinations(pos_groups[k].index, position_limits[k])) for k in position_limits]\n",
    "#     all_combos = product(*pos_combos)\n",
    "#     best_combo = None\n",
    "#     best_points = -float(\"inf\")\n",
    "#     for combo in all_combos:\n",
    "#         idxs = [i for group in combo for i in group]\n",
    "#         team = pdf.loc[list(idxs)]\n",
    "#         if team.groupby(\"team_season_key\").size().max() > max_team_players:\n",
    "#             continue\n",
    "#         if team[\"current_value\"].sum() > max_total_value:\n",
    "#             continue\n",
    "#         points = team[\"predicted_total_points\"].sum()\n",
    "#         if points > best_points:\n",
    "#             best_points = points\n",
    "#             best_combo = team\n",
    "#     if best_combo is not None:\n",
    "#         return best_combo\n",
    "#     return pd.DataFrame(columns=pdf.columns)\n",
    "\n",
    "# totw_df = fpl_df_filtered.groupBy(\"gameweek_key\").applyInPandas(select_totw, schema=fpl_df_filtered.schema)\n",
    "\n",
    "# display(totw_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7691805492916153,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dim_fact_weekly",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}