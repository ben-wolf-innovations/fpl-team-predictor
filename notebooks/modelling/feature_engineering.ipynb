{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a4c0f2-682b-42a1-99d2-921de99aa576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6665d485-91dd-4208-a040-a33a7417f82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0124c1-d2d4-414e-9c0e-d34fdf389620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = False,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de311b28-8b2f-4489-a9ea-231eda0d8f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9b727c8-9a94-4831-87dc-f93c6fa9f13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bf92f2a-7d7a-49b7-ae80-0717732e0b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "try:\n",
    "    PROTOCOL = dbutils.widgets.get(\"PROTOCOL\")\n",
    "except Exception:\n",
    "    PROTOCOL = \"HIST\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "valid_protocols = {\"HIST\", \"INCR\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "\n",
    "# Validate PROTOCOL\n",
    "if PROTOCOL not in valid_protocols:\n",
    "    print(f\"Invalid PROTOCOL: {PROTOCOL}. Must be one of {valid_protocols}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid PROTOCOL\")\n",
    "    \n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "feature_schema = f\"fpl_feature_{ENV}\"\n",
    "rolling_window_size = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2671630-1582-446e-af11-ec7400dd928b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- season_key: integer (nullable = true)\n |-- team_key: integer (nullable = true)\n |-- team_id: integer (nullable = true)\n |-- team_name: string (nullable = true)\n |-- team_name_short: string (nullable = true)\n |-- is_promoted: boolean (nullable = true)\n |-- is_relegated: boolean (nullable = true)\n |-- last_updated: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(f\"{silver_schema}.teams\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e700c647-be10-4887-83c1-76136462841a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load Source Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b94650d-0899-47a1-9ec5-24c4d9b3eaaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fixtures_df = spark.table(f\"{silver_schema}.fixtures\").filter(F.col(\"home_team_score\").isNotNull())\n",
    "teams_df = spark.table(f\"{silver_schema}.teams\")\n",
    "stats_df = spark.table(f\"{silver_schema}.gameweek_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e51f52-3148-4d41-9adc-014960d84f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Team Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56331802-f912-4449-9be8-4425bf1034b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Aggregate xG, xA, and exp_stats_available per team per fixture\n",
    "team_xg_xa_df = stats_df.groupBy(\"fixture_key\", \"team_key\").agg(\n",
    "    F.sum(\"expected_goals\").alias(\"team_expected_goals\"),\n",
    "    F.sum(\"expected_assists\").alias(\"team_expected_assists\"),\n",
    "    F.max(\"exp_stats_available\").alias(\"team_exp_stats_available\")\n",
    ").withColumn(\n",
    "    \"team_expected_goal_involvements\", F.col(\"team_expected_goals\") + F.col(\"team_expected_assists\")\n",
    ")\n",
    "\n",
    "#Create opponent xG/xA aggregates\n",
    "opponent_xg_xa_df = team_xg_xa_df.select(\n",
    "    F.col(\"fixture_key\"),\n",
    "    F.col(\"team_key\").alias(\"opponent_team_key\"),\n",
    "    F.col(\"team_expected_goals\").alias(\"expected_goals_against\"),\n",
    "    F.col(\"team_expected_assists\").alias(\"expected_assists_against\"),\n",
    "    F.col(\"team_expected_goal_involvements\").alias(\"expected_goal_involvements_against\")\n",
    ")\n",
    "\n",
    "#Step 3: Transform fixtures into team-level records\n",
    "home_df = fixtures_df.select(\n",
    "    F.col(\"fixture_key\"),\n",
    "    F.col(\"season_key\"),\n",
    "    F.col(\"gameweek_key\"),\n",
    "    F.col(\"home_team_key\").alias(\"team_key\"),\n",
    "    F.col(\"away_team_key\").alias(\"opponent_team_key\"),\n",
    "    F.lit(True).alias(\"is_home\"),\n",
    "    F.col(\"home_team_score\").alias(\"goals_for\"),\n",
    "    F.col(\"away_team_score\").alias(\"goals_against\")\n",
    ")\n",
    "\n",
    "away_df = fixtures_df.select(\n",
    "    F.col(\"fixture_key\"),\n",
    "    F.col(\"season_key\"),\n",
    "    F.col(\"gameweek_key\"),\n",
    "    F.col(\"away_team_key\").alias(\"team_key\"),\n",
    "    F.col(\"home_team_key\").alias(\"opponent_team_key\"),\n",
    "    F.lit(False).alias(\"is_home\"),\n",
    "    F.col(\"away_team_score\").alias(\"goals_for\"),\n",
    "    F.col(\"home_team_score\").alias(\"goals_against\")\n",
    ")\n",
    "\n",
    "team_fixtures_df = home_df.unionByName(away_df)\n",
    "\n",
    "#Add match-level metrics\n",
    "team_fixtures_df = team_fixtures_df.withColumn(\n",
    "    \"goal_diff\", F.col(\"goals_for\") - F.col(\"goals_against\")\n",
    ").withColumn(\n",
    "    \"match_points\",\n",
    "    F.when(F.col(\"goals_for\") > F.col(\"goals_against\"), F.lit(3))\n",
    "     .when(F.col(\"goals_for\") == F.col(\"goals_against\"), F.lit(1))\n",
    "     .otherwise(F.lit(0))\n",
    ")\n",
    "\n",
    "#Join team xG/xA and opponent xG/xA\n",
    "team_fixtures_df = team_fixtures_df.join(\n",
    "    team_xg_xa_df,\n",
    "    on=[\"fixture_key\", \"team_key\"],\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    opponent_xg_xa_df,\n",
    "    on=[\"fixture_key\", \"opponent_team_key\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#Compute rolling metrics ---\n",
    "rolling_window = Window.partitionBy(\"team_key\", \"season_key\").orderBy(\"gameweek_key\").rowsBetween(-rolling_window_size + 1, 0)\n",
    "\n",
    "team_fixtures_df = (\n",
    "    team_fixtures_df.withColumn(\"rolling_points\", F.sum(\"match_points\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_goal_diff\", F.sum(\"goal_diff\").over(rolling_window)) \n",
    "    .withColumn(\"home_rolling_points\", F.sum(F.when(F.col(\"is_home\"), F.col(\"match_points\")).otherwise(0)).over(rolling_window)) \n",
    "    .withColumn(\"away_rolling_points\", F.sum(F.when(~F.col(\"is_home\"), F.col(\"match_points\")).otherwise(0)).over(rolling_window)) \n",
    "    .withColumn(\"rolling_team_expected_goals\", F.sum(\"team_expected_goals\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_team_expected_assists\", F.sum(\"team_expected_assists\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_team_expected_goal_involvements\", F.sum(\"team_expected_goal_involvements\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_expected_goals_against\", F.sum(\"expected_goals_against\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_expected_assists_against\", F.sum(\"expected_assists_against\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_expected_goal_involvements_against\", F.sum(\"expected_goal_involvements_against\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_goal_difference\", F.sum(\"goal_diff\").over(rolling_window)) \n",
    "    .withColumn(\"rolling_games_played\", F.count(\"fixture_key\").over(rolling_window)) \n",
    "    .withColumn(\"avg_team_expected_goals\", F.round(F.col(\"rolling_team_expected_goals\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_team_expected_assists\", F.round(F.col(\"rolling_team_expected_assists\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_team_expected_goal_involvements\", F.round(F.col(\"rolling_team_expected_goal_involvements\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_expected_goals_against\", F.round(F.col(\"rolling_expected_goals_against\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_expected_assists_against\", F.round(F.col(\"rolling_expected_assists_against\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_expected_goal_involvements_against\", F.round(F.col(\"rolling_expected_goal_involvements_against\") / F.col(\"rolling_games_played\"), 3)) \n",
    "    .withColumn(\"avg_goal_difference\", F.round(F.col(\"rolling_goal_difference\") / F.col(\"rolling_games_played\"), 3))\n",
    ")\n",
    "\n",
    "#Join team metadata ---\n",
    "team_features_df = team_fixtures_df.join(\n",
    "    teams_df.select(\"team_key\", \"team_name\", \"team_name_short\", \"is_promoted\", \"is_relegated\", \"season_key\"),\n",
    "    on=[\"team_key\", \"season_key\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "#Select final columns ---\n",
    "team_features_df = team_features_df.select(\n",
    "    \"team_key\", \"team_name\", \"team_name_short\", \"season_key\", \"gameweek_key\",\n",
    "    \"is_home\", \"goals_for\", \"goals_against\", \"goal_diff\", \"match_points\",\n",
    "    \"team_expected_goals\", \"team_expected_assists\", \"team_expected_goal_involvements\",\n",
    "    \"expected_goals_against\", \"expected_assists_against\", \"expected_goal_involvements_against\",\n",
    "    \"team_exp_stats_available\",\n",
    "    \"rolling_points\", \"rolling_goal_diff\", \"home_rolling_points\", \"away_rolling_points\",\n",
    "    \"rolling_team_expected_goals\", \"rolling_team_expected_assists\", \"rolling_team_expected_goal_involvements\",\n",
    "    \"rolling_expected_goals_against\", \"rolling_expected_assists_against\", \"rolling_expected_goal_involvements_against\",\n",
    "    \"rolling_goal_difference\", \"rolling_games_played\",\n",
    "    \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\",\n",
    "    \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\",\n",
    "    \"avg_goal_difference\",\n",
    "    \"is_promoted\", \"is_relegated\"\n",
    ")\n",
    "\n",
    "write_to_table(\n",
    "    df = team_features_df,\n",
    "    table_name = f\"{feature_schema}.team_features\",\n",
    "    mode = \"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9969aa1-087f-42c9-a574-9b0d3a025ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}