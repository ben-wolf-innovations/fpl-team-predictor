{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "139dc046-9af3-4555-8bc0-d929c63fcaf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ea1e87-dde9-4496-b545-2ef41b34ce47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENV = \"dev\"\n",
    "\n",
    "feature_schema = f\"fpl_feature_{ENV}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b305643d-4747-4351-9b78-b2d9aa2844b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the player_features table and convert to pandas\n",
    "player_features_df = spark.read.table(f\"{feature_schema}.player_features\").filter(\n",
    "    F.col(\"season_key\") >= 202122\n",
    ")\n",
    "\n",
    "player_features_pd = player_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffc33e6-2771-4639-aa91-897b19253fd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assemble features and handle missing values"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_cols = [\n",
    "    \"was_home\", \"rolling_expected_goals\", \"rolling_expected_assists\", \"rolling_expected_goal_involvements\", \"rolling_goals_scored\", \"rolling_assists\", \"rolling_total_points\", \"rolling_minutes\", \"rolling_clean_sheets\", \"rolling_bps\", \"rolling_ict_index\", \"rolling_influence\", \"rolling_creativity\", \"rolling_threat\", \"rolling_defensive_contribution\", \"rolling_clearances_blocks_interceptions\", \"rolling_bonus\", \"rolling_saves\", \"rolling_games_played\", \"rolling_minutes_points\", \"rolling_assist_points\", \"rolling_goal_points\", \"rolling_clean_sheet_points\", \"rolling_defensive_contribution_points\", \"rolling_penalty_miss_points\", \"rolling_goals_conceded_points\", \"rolling_yellow_card_points\", \"rolling_red_card_points\", \"rolling_own_goal_points\", \"avg_expected_goals\", \"avg_expected_assists\", \"avg_expected_goal_involvements\", \"avg_goals_scored\", \"avg_assists\", \"avg_total_points\", \"avg_minutes\", \"avg_clean_sheets\", \"avg_bps\", \"avg_ict_index\", \"avg_influence\", \"avg_creativity\", \"avg_threat\", \"avg_defensive_contribution\", \"avg_clearances_blocks_interceptions\", \"avg_bonus\", \"avg_saves\", \"avg_minutes_points\", \"avg_assist_points\", \"avg_goal_points\", \"avg_clean_sheet_points\", \"avg_defensive_contribution_points\", \"avg_penalty_miss_points\", \"avg_goals_conceded_points\", \"avg_yellow_card_points\", \"avg_red_card_points\", \"avg_own_goal_points\", \"rolling_points\", \"rolling_team_expected_goals\", \"rolling_expected_goals_against\", \"rolling_goal_difference\", \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\", \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\", \"avg_goal_difference\", \"match_points\", \"team_rolling_goals_conceded\", \"team_rolling_goal_difference\", \"player_share_of_team_xG\", \"player_share_of_team_points\", \"opponent_rolling_points\", \"opponent_rolling_team_expected_goals\", \"opponent_rolling_expected_goals_against\", \"opponent_rolling_goal_difference\", \"opponent_avg_team_expected_goals\", \"opponent_avg_team_expected_assists\", \"opponent_avg_team_expected_goal_involvements\", \"opponent_avg_expected_goals_against\", \"opponent_avg_expected_assists_against\", \"opponent_avg_expected_goal_involvements_against\", \"opponent_avg_goal_difference\"\n",
    "]\n",
    "\n",
    "# Fill missing values and cast to float\n",
    "player_features_pd[feature_cols] = player_features_pd[feature_cols].fillna(-1)\n",
    "player_features_pd[feature_cols] = player_features_pd[feature_cols].astype(float)\n",
    "\n",
    "target_col = \"total_points\"\n",
    "\n",
    "# Show a sample of the feature matrix\n",
    "player_features_pd[feature_cols + [target_col]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3dc83fc-e3a2-494e-b671-cc4d8f0cd904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = player_features_pd[feature_cols]\n",
    "y = player_features_pd[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set count: {len(X_train)}\")\n",
    "print(f\"Test set count: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ba088b4-6083-4810-9bfe-3c8e8dd4440a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train and log multiple regression models"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"RandomForest\", RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    (\"GradientBoosting\", GradientBoostingRegressor(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    (\"Ridge\", Ridge(alpha=1.0)),\n",
    "    (\"Lasso\", Lasso(alpha=0.1)),\n",
    "    (\"SVR\", SVR(C=1.0, epsilon=0.2))\n",
    "]\n",
    "\n",
    "client = MlflowClient()\n",
    "results = []\n",
    "for name, model in models:\n",
    "    with mlflow.start_run(run_name=f\"{name}_regression\") as run:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = mean_squared_error(y_test, y_pred) ** 0.5  # Fix: remove 'squared', take sqrt\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        input_example = X_train.head(5)\n",
    "        output_example = model.predict(X_train.head(5))\n",
    "        signature = infer_signature(input_example, output_example)\n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            for param, value in model.get_params().items():\n",
    "                mlflow.log_param(param, value)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example\n",
    "        )\n",
    "        run_id = run.info.run_id\n",
    "        model_name = f\"FPL_TotalPoints_{name}_SKLearn_21\"\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        result = mlflow.register_model(model_uri, model_name)\n",
    "        results.append((name, rmse, r2, result.version))\n",
    "        print(f\"{name} model registered as '{model_name}' with version: {result.version}. RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "print(\"All models trained and registered.\")\n",
    "for name, rmse, r2, version in results:\n",
    "    print(f\"{name}: RMSE={rmse:.4f}, R2={r2:.4f}, Version={version}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}