{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9fc03b-633c-42e5-b1c5-5288fe579bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5d6473-ea6c-47f4-bf6d-9f36ccfa3d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1a7bb1-4bee-4e64-a2d7-cbe1ee76d09a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7140b5e7-7a89-4c1f-abdf-4d39ee2fba9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5582bce5-e8a2-40b9-8561-2bb2299ae4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalise_season_keys(season_start: int) -> dict:\n",
    "    start_short = str(season_start)[-2:]\n",
    "    end_short = str(season_start + 1)[-2:]\n",
    "\n",
    "    return {\n",
    "        \"season_key\": f\"{season_start}{end_short}\",      # e.g. \"201617\"\n",
    "        \"season_table_suffix\": f\"{start_short}_{end_short}\",  # e.g. \"16_17\"\n",
    "        \"season_short\": f\"{start_short}{end_short}\"     # e.g. \"1617\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61f3f73-d1c3-4701-99a5-7d9b706678c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd762a04-71f3-44e7-a823-51b61d14c16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "try:\n",
    "    PROTOCOL = dbutils.widgets.get(\"PROTOCOL\")\n",
    "except Exception:\n",
    "    PROTOCOL = \"INCR\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "valid_protocols = {\"HIST\", \"INCR\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "\n",
    "# Validate PROTOCOL\n",
    "if PROTOCOL not in valid_protocols:\n",
    "    print(f\"Invalid PROTOCOL: {PROTOCOL}. Must be one of {valid_protocols}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid PROTOCOL\")\n",
    "    \n",
    "bronze_schema = f\"fpl_bronze_{ENV}\"\n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "CURRENT_SEASON_START = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa8ae2f-4202-416d-bd18-c023eefd3212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Define Stats Schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "453957d8-aefd-4647-bbf7-19487df19820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "canonical_schema = {\n",
    "    \"element\": IntegerType(),\n",
    "    \"assists\": IntegerType(),\n",
    "    \"bonus\": IntegerType(),\n",
    "    \"bps\": IntegerType(),\n",
    "    \"clean_sheets\": IntegerType(),\n",
    "    \"clearances_blocks_interceptions\": IntegerType(),\n",
    "    \"creativity\": DoubleType(),\n",
    "    \"defensive_contribution\": IntegerType(),\n",
    "    \"element\": IntegerType(),\n",
    "    \"expected_assists\": DoubleType(),\n",
    "    \"expected_goal_involvements\": DoubleType(),\n",
    "    \"expected_goals\": DoubleType(),\n",
    "    \"expected_goals_conceded\": DoubleType(),\n",
    "    \"fixture\": IntegerType(),\n",
    "    \"goals_conceded\": IntegerType(),\n",
    "    \"goals_scored\": IntegerType(),\n",
    "    \"ict_index\": DoubleType(),\n",
    "    \"influence\": DoubleType(),\n",
    "    \"minutes\": IntegerType(),\n",
    "    \"own_goals\": IntegerType(),\n",
    "    \"penalties_missed\": IntegerType(),\n",
    "    \"penalties_saved\": IntegerType(),\n",
    "    \"recoveries\": IntegerType(),\n",
    "    \"red_cards\": IntegerType(),\n",
    "    \"saves\": IntegerType(),\n",
    "    \"starts\": IntegerType(),\n",
    "    \"tackles\": IntegerType(),\n",
    "    \"threat\": DoubleType(),\n",
    "    \"total_points\": IntegerType(),\n",
    "    \"value\": IntegerType(),\n",
    "    \"was_home\": BooleanType(),\n",
    "    \"yellow_cards\": IntegerType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744c2476-d8aa-4ecb-91b1-452667821115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_gameweek_stats_df(bronze_schema: str, silver_schema:str, season: int) -> DataFrame:\n",
    "\n",
    "    season_keys = normalise_season_keys(season)\n",
    "    season_suffix = season_keys[\"season_table_suffix\"]\n",
    "    season_key = season_keys[\"season_key\"]\n",
    "\n",
    "    raw_df = spark.table(f\"{bronze_schema}.player_gameweek_stats_{season_suffix}\")\n",
    "\n",
    "    raw_df = raw_df.withColumns(\n",
    "        {\n",
    "            \"exp_stats_available\": F.lit(\"expected_goals\" in raw_df.columns),\n",
    "            \"def_con_available\": F.lit(\"defensive_contribution\" in raw_df.columns),\n",
    "            \"season_key\": F.lit(season_key).cast(\"int\")\n",
    "        }\n",
    "        )\n",
    "\n",
    "    # Add missing columns with nulls\n",
    "    for col_name, col_type in canonical_schema.items():\n",
    "        if col_name not in raw_df.columns:\n",
    "            raw_df = raw_df.withColumn(col_name, F.lit(None).cast(col_type))\n",
    "        else:\n",
    "            raw_df = raw_df.withColumn(col_name, raw_df[col_name].cast(col_type))\n",
    "\n",
    "    raw_df = raw_df.select(\n",
    "        list(canonical_schema.keys()) + [\"exp_stats_available\", \"def_con_available\", \"season_key\"]\n",
    "                           ).withColumnRenamed(\n",
    "                               \"element\", \"player_id\"\n",
    "                           )\n",
    "\n",
    "    #get fixture data\n",
    "\n",
    "    raw_df = raw_df.withColumn(\n",
    "        \"fixture_key\",\n",
    "        F.concat(F.col(\"season_key\").cast(\"string\"), F.lpad(F.col(\"fixture\").cast(\"string\"), 3, \"0\"))\n",
    "    )\n",
    "\n",
    "    fixtures_df = spark.table(f\"{silver_schema}.fixtures\")\n",
    "\n",
    "    fixtures_raw_df = raw_df.join(\n",
    "        fixtures_df.select(\"fixture_key\", \"home_team_key\", \"away_team_key\",\n",
    "                            \"home_team_score\", \n",
    "                            \"away_team_score\", \n",
    "                            \"gameweek_key\"),\n",
    "        on=\"fixture_key\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    #get player data\n",
    "    players_raw_df = fixtures_raw_df.withColumn(\n",
    "        \"player_season_key\",\n",
    "        F.concat(F.col(\"season_key\").cast(\"string\"), F.lpad(F.col(\"player_id\").cast(\"string\"), 3, \"0\")).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    players_df = spark.table(f\"{silver_schema}.players\").select(\n",
    "            F.col(\"player_season_key\").alias(\"_player_season_key\"), \n",
    "            \"team_key\", \n",
    "            \"player_key\", \n",
    "            \"position_key\", \n",
    "            \"first_fixture_key\", \n",
    "            \"last_fixture_key\")\n",
    "\n",
    "    gameweek_stats_df = players_raw_df.join(\n",
    "        players_df,\n",
    "        on=(\n",
    "            (players_raw_df[\"player_season_key\"] == players_df[\"_player_season_key\"]) &\n",
    "            (players_raw_df[\"fixture_key\"] >= players_df[\"first_fixture_key\"]) &\n",
    "            (players_raw_df[\"fixture_key\"] <= players_df[\"last_fixture_key\"])\n",
    "        ),\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    gameweek_stats_df = gameweek_stats_df.withColumns(\n",
    "        {\n",
    "            \"opponent_team_key\": F.when(F.col(\"was_home\"), F.col(\"away_team_key\")).otherwise(F.col(\"home_team_key\")),\n",
    "            \"team_score\": F.when(F.col(\"was_home\"), F.col(\"home_team_score\")).otherwise(F.col(\"away_team_score\")),\n",
    "            \"opponent_score\": F.when(F.col(\"was_home\"), F.col(\"away_team_score\")).otherwise(F.col(\"home_team_score\")),\n",
    "            \"player_fixture_key\": F.concat(F.col(\"player_key\"), F.col(\"fixture_key\")).cast(\"long\")\n",
    "        }\n",
    "        ).drop(\n",
    "            \"home_team_key\", \"away_team_key\", \"home_team_score\", \"away_team_score\", \n",
    "            \"first_fixture_key\", \"last_fixture_key\", \"_player_season_key\"\n",
    "        )\n",
    "\n",
    "    return gameweek_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e62df9-e4a0-4450-a409-701c99b61687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_player_stats_df = []\n",
    "\n",
    "if PROTOCOL == \"HIST\":\n",
    "    for season in range(2016, CURRENT_SEASON_START + 1): # 2016 to 2025 inclusive\n",
    "\n",
    "        df = transform_gameweek_stats_df(bronze_schema, silver_schema, season)\n",
    "\n",
    "        all_player_stats_df.append(df)\n",
    "\n",
    "    player_stats_df = all_player_stats_df[0]\n",
    "    for df in all_player_stats_df[1:]:\n",
    "        player_stats_df = player_stats_df.unionByName(df)\n",
    "\n",
    "    write_to_table(\n",
    "        df = player_stats_df,\n",
    "        table_name = f\"{silver_schema}.gameweek_stats\",\n",
    "        mode = \"overwrite\",\n",
    "        merge_schema = False\n",
    "    )\n",
    "\n",
    "elif PROTOCOL == \"INCR\":\n",
    "    season = CURRENT_SEASON_START\n",
    "\n",
    "    player_stats_df = transform_gameweek_stats_df(bronze_schema, silver_schema, season)\n",
    "\n",
    "    merge_to_table(\n",
    "        df = player_stats_df,\n",
    "        table_name = f\"{silver_schema}.gameweek_stats\",\n",
    "        merge_condition = \"source.player_fixture_key = target.player_fixture_key\",\n",
    "        spark = spark\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tf_player_gameweek_stats",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}