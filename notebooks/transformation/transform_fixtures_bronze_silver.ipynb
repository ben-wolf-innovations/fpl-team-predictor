{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48fe9cbc-8c49-4c38-87e1-5a28a68560ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dc0d07-3deb-4a66-bfa2-44b2167c37b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4be6024f-9c74-4a81-bd6c-a45b8da0c476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10d4572f-7874-494b-8156-c01785fdd7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eaa9e98-60cf-48eb-9c56-ec1a444a4ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and normalise players\n",
    "def load_and_normalise_players(\n",
    "    bronze_schema: str, \n",
    "    season: str):\n",
    "\n",
    "    df = spark.table(f\"{bronze_schema}.players_raw_{season}\")\n",
    "    for col in expected_players_cols:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None).cast(\"string\"))\n",
    "    return df.select(*expected_players_cols)\n",
    "\n",
    "# Load and normalise stats\n",
    "def load_and_normalise_stats(\n",
    "    bronze_schema: str, \n",
    "    season: str):\n",
    "\n",
    "    df = spark.table(f\"{bronze_schema}.player_gameweek_stats_{season}\")\n",
    "    for col in expected_stats_cols:\n",
    "        if col not in df.columns:\n",
    "            df = df.withColumn(col, F.lit(None))\n",
    "    return df.select(*expected_stats_cols)\n",
    "\n",
    "# Extract fixtures from legacy source\n",
    "def extract_unique_fixtures_historic(\n",
    "    player_stats_df, \n",
    "    players_df, \n",
    "    season_key):\n",
    "\n",
    "    stats_with_team = player_stats_df.join(\n",
    "        players_df.withColumnRenamed(\"id\", \"element\"),\n",
    "        on=\"element\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    home_df = stats_with_team.filter(F.col(\"was_home\") == True).select(\n",
    "        \"fixture\",\n",
    "        F.col(\"team_code\").alias(\"home_team_key\"),\n",
    "        F.col(\"team_h_score\").alias(\"home_team_score\"),\n",
    "        F.col(\"team_a_score\").alias(\"away_team_score\")\n",
    "    )\n",
    "\n",
    "    away_df = stats_with_team.filter(F.col(\"was_home\") == False).select(\n",
    "        \"fixture\",\n",
    "        F.col(\"team_code\").alias(\"away_team_key\")\n",
    "    )\n",
    "\n",
    "    base_info_df = player_stats_df.select(\n",
    "        \"fixture\", \"round\", \"kickoff_time\"\n",
    "    ).dropDuplicates([\"fixture\"])\n",
    "\n",
    "    fixtures_df = home_df.join(away_df, \n",
    "                               on=\"fixture\", \n",
    "                               how=\"inner\"\n",
    "                            ).join(\n",
    "                                base_info_df, \n",
    "                                on=\"fixture\", \n",
    "                                how=\"left\"\n",
    "                            ).dropDuplicates([\"fixture\"])\n",
    "\n",
    "    fixtures_df = fixtures_df.withColumn(\n",
    "            \"season_key\", \n",
    "            F.lit(season_key).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"fixture_id\", \n",
    "            F.col(\"fixture\")\n",
    "        ).withColumn(\n",
    "            \"gameweek_key\", \n",
    "            F.concat(F.lit(season_key), F.lpad(F.col(\"round\").cast(\"string\"), 2, \"0\")).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"fixture_key\", \n",
    "            F.concat(F.lit(season_key), F.lpad(F.col(\"fixture_id\").cast(\"string\"), 3, \"0\")).cast(\"int\")\n",
    "        )\n",
    "\n",
    "    return fixtures_df.select(\n",
    "        \"fixture_key\", \"season_key\", \"gameweek_key\", \"home_team_key\", \"away_team_key\",\n",
    "        \"fixture_id\", \"home_team_score\", \"away_team_score\", \"kickoff_time\", \"round\"\n",
    "    )\n",
    "\n",
    "# Extract fixtures from modern source\n",
    "def extract_fixtures_from_api(\n",
    "    bronze_schema: str, \n",
    "    silver_schema: str, \n",
    "    season: str, \n",
    "    season_key: str):\n",
    "\n",
    "    fixtures_df = spark.table(f\"{bronze_schema}.fixtures_{season}\")\n",
    "    teams_df = spark.table(f\"{silver_schema}.teams\").filter(F.col(\"season_key\") == F.lit(season_key))\n",
    "\n",
    "    teams_home = teams_df.select(\n",
    "        F.col(\"team_id\").alias(\"team_h\"),\n",
    "        F.col(\"team_key\").alias(\"home_team_key\")\n",
    "    )\n",
    "    teams_away = teams_df.select(\n",
    "        F.col(\"team_id\").alias(\"team_a\"),\n",
    "        F.col(\"team_key\").alias(\"away_team_key\")\n",
    "    )\n",
    "\n",
    "    fixtures_df = fixtures_df.join(\n",
    "            teams_home, \n",
    "            on=\"team_h\", \n",
    "            how=\"left\"\n",
    "        ).join(\n",
    "            teams_away, \n",
    "            on=\"team_a\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # Filter for finished fixtures\n",
    "    finished_df = fixtures_df.filter(F.col(\"finished\") == True)\n",
    "\n",
    "    # Explode stats array\n",
    "    exploded_stats = finished_df.select(\n",
    "        \"id\", \"kickoff_time\", \"home_team_key\", \"away_team_key\", \"finished\",\n",
    "        F.explode(\"stats\").alias(\"stat\")\n",
    "    ).filter(\n",
    "        F.col(\"stat.identifier\") == \"goals_scored\"\n",
    "    )\n",
    "\n",
    "    # Explode home and away arrays\n",
    "    home_goals = exploded_stats.select(\n",
    "        \"id\",\n",
    "        F.explode(\"stat.h\").alias(\"home_stat\")\n",
    "    ).groupBy(\"id\").agg(F.sum(\"home_stat.value\").cast(\"int\").alias(\"home_team_score\"))\n",
    "\n",
    "    away_goals = exploded_stats.select(\n",
    "        \"id\",\n",
    "        F.explode(\"stat.a\").alias(\"away_stat\")\n",
    "    ).groupBy(\"id\").agg(F.sum(\"away_stat.value\").cast(\"int\").alias(\"away_team_score\"))\n",
    "\n",
    "    # Join scores back to fixtures\n",
    "    scored_df = fixtures_df.join(\n",
    "            home_goals, \n",
    "            on=\"id\", \n",
    "            how=\"left\"\n",
    "        ).join(\n",
    "            away_goals, \n",
    "            on=\"id\", \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # Add keys and fix null scores\n",
    "    scored_df = scored_df.withColumn(\n",
    "        \"season_key\", \n",
    "        F.lit(season_key).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"fixture_id\", \n",
    "            F.col(\"id\")\n",
    "        ).withColumn(\n",
    "            \"round\", \n",
    "            F.col(\"event\")\n",
    "        ).withColumn(\n",
    "            \"gameweek_key\", \n",
    "            F.concat(F.lit(season_key), F.lpad(F.col(\"event\").cast(\"string\"), 2, \"0\")).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"fixture_key\", \n",
    "            F.concat(F.lit(season_key), F.lpad(F.col(\"id\").cast(\"string\"), 3, \"0\")).cast(\"int\")\n",
    "        ).withColumn(\n",
    "            \"home_team_score\", \n",
    "            F.when(F.col(\"finished\") & F.col(\"home_team_score\").isNull(), F.lit(0)).otherwise(F.col(\"home_team_score\"))\n",
    "        ).withColumn(\n",
    "            \"away_team_score\", \n",
    "            F.when(F.col(\"finished\") & F.col(\"away_team_score\").isNull(), F.lit(0)).otherwise(F.col(\"away_team_score\"))\n",
    "        )\n",
    "\n",
    "    return scored_df.select(\n",
    "        \"fixture_key\", \n",
    "        \"season_key\", \n",
    "        \"gameweek_key\", \n",
    "        \"home_team_key\", \n",
    "        \"away_team_key\",\n",
    "        \"fixture_id\", \n",
    "        \"home_team_score\", \n",
    "        \"away_team_score\", \n",
    "        \"kickoff_time\", \n",
    "        \"round\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480b6828-650f-4fa3-a90a-6d90db1ffddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "679648df-862f-4b77-bc41-838c8d07b31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "try:\n",
    "    PROTOCOL = dbutils.widgets.get(\"PROTOCOL\")\n",
    "except Exception:\n",
    "    PROTOCOL = \"INCR\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "valid_protocols = {\"HIST\", \"INCR\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "\n",
    "# Validate PROTOCOL\n",
    "if PROTOCOL not in valid_protocols:\n",
    "    print(f\"Invalid PROTOCOL: {PROTOCOL}. Must be one of {valid_protocols}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid PROTOCOL\")\n",
    "    \n",
    "bronze_schema = f\"fpl_bronze_{ENV}\"\n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "CURRENT_SEASON = \"25_26\"\n",
    "API_SEASONS = [\"25_26\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b2408a-c23a-4240-be57-1c0fc56443f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Transform Fixtures and write to {silver_schema}.fixtures\n",
    "\n",
    "Split into 2 ETL patterns, historic and api.\n",
    "\n",
    "If HIST protocol, need to get historic seasons as well as current season. Data for previous seasons found in player gameweek stats, getting team data from the players_raw datasets.\n",
    "\n",
    "Historic method uses players_raw (with player id and player team_code), player_gameweek_stats for each gameweek stats per player to get a full fixture. \n",
    "\n",
    "API uses the fixtures data from bootstrap-static API data and {silver_schema}.teams to get fixture schema.\n",
    "\n",
    "Can be run HIST (get all fixtures) or INCR (only current season).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4dd5eda-1eda-47f9-b90c-eb72f44bff8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running INCR protocol for season: 25_26\n"
     ]
    }
   ],
   "source": [
    "# Define expected columns for legacy seasons\n",
    "expected_players_cols = [\"id\", \"team_code\"]\n",
    "expected_stats_cols = [\n",
    "    \"element\", \"round\", \"fixture\", \"opponent_team\", \"was_home\",\n",
    "    \"team_a_score\", \"team_h_score\", \"kickoff_time\"\n",
    "]\n",
    "\n",
    "# Season mapping\n",
    "season_keys = {\n",
    "    \"16_17\": \"201617\",\n",
    "    \"17_18\": \"201718\",\n",
    "    \"18_19\": \"201819\",\n",
    "    \"19_20\": \"201920\",\n",
    "    \"20_21\": \"202021\",\n",
    "    \"21_22\": \"202122\",\n",
    "    \"22_23\": \"202223\",\n",
    "    \"23_24\": \"202324\",\n",
    "    \"24_25\": \"202425\",\n",
    "    \"25_26\": \"202526\"\n",
    "}\n",
    "\n",
    "# Run HIST or INCR protocol\n",
    "if PROTOCOL == \"HIST\":\n",
    "    fixtures_all = []\n",
    "\n",
    "    for season, season_key in season_keys.items():\n",
    "        print(f\"Processing season: {season}\")\n",
    "        \n",
    "        if season in API_SEASONS:  # Extend this list for future seasons\n",
    "            fixtures_df = extract_fixtures_from_api(\n",
    "                bronze_schema = bronze_schema,\n",
    "                silver_schema = silver_schema,\n",
    "                season = season,\n",
    "                season_key = season_key\n",
    "            )\n",
    "        else:\n",
    "            players_df = load_and_normalise_players(bronze_schema, season)\n",
    "            player_stats_df = load_and_normalise_stats(bronze_schema, season)\n",
    "\n",
    "            fixtures_df = extract_unique_fixtures_historic(player_stats_df, players_df, season_key)\n",
    "\n",
    "        fixtures_all.append(fixtures_df)\n",
    "\n",
    "    final_fixtures_df = fixtures_all[0]\n",
    "    for df in fixtures_all[1:]:\n",
    "        final_fixtures_df = final_fixtures_df.unionByName(df)\n",
    "\n",
    "    write_to_table(\n",
    "        df = final_fixtures_df,\n",
    "        table_name = f\"{silver_schema}.fixtures\",\n",
    "        mode =  \"overwrite\",\n",
    "        merge_schema = False\n",
    "    ) \n",
    "\n",
    "elif PROTOCOL == \"INCR\":\n",
    "    season_key = season_keys[CURRENT_SEASON]\n",
    "\n",
    "    print(f\"Running INCR protocol for season: {season}\")\n",
    "    incr_df = extract_fixtures_from_api(\n",
    "        bronze_schema = bronze_schema,\n",
    "        silver_schema = silver_schema,\n",
    "        season = season,\n",
    "        season_key = season_key\n",
    "    )\n",
    "\n",
    "    merge_to_table(\n",
    "        df = incr_df,\n",
    "        table_name = f\"{silver_schema}.fixtures\",\n",
    "        merge_condition = \"target.fixture_key = source.fixture_key\",\n",
    "        spark = spark\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_fixtures_bronze_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}