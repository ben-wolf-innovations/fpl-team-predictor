{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108a0ccb-d92b-43c2-bb8e-111618b90798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298b9d3d-df62-4638-9e72-ee079997fe5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfa7a27-52fb-44fc-8e69-4697ca5dcc9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22df1e9-6d40-439d-8cbd-881ed1062844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "#ensure valid ENV\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "    \n",
    "bronze_schema = f\"fpl_bronze_{ENV}\"\n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "\n",
    "fixtures_df = spark.table(f\"{silver_schema}.fixtures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b07587-5e2b-43ad-9f82-539dd8d4ca24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Write Seasons\n",
    "\n",
    "Get seasons from fixtures table in silver layer. \n",
    "\n",
    "Currently has season key e.g. 202526, season_short (25_26) and first/last kick off times per season. \n",
    "\n",
    "Possible additions to add season results in future? Winner, Champions league, relegated etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219585ab-6e45-4a18-aab6-1faba72031c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seasons_df = fixtures_df.select(\n",
    "        \"season_key\", \n",
    "        \"kickoff_time\"\n",
    "    ).withColumn(\n",
    "        \"season_short\",\n",
    "        F.concat_ws(\"_\",\n",
    "            F.col(\"season_key\").substr(3, 2),  # '20' from '202021'\n",
    "            F.col(\"season_key\").substr(5, 2)   # '21' from '202021'\n",
    "        )\n",
    "    ).groupBy(\"season_key\", \"season_short\"\n",
    "    ).agg(\n",
    "        F.min(\"kickoff_time\").alias(\"season_start\"),\n",
    "        F.max(\"kickoff_time\").alias(\"season_end\")\n",
    "    )\n",
    "\n",
    "write_to_table(\n",
    "    df = seasons_df,\n",
    "    table_name = f\"{silver_schema}.seasons\",\n",
    "    mode = \"overwrite\",\n",
    "    merge_schema  = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735ce64c-b45b-4a19-ac1f-cefa279ab90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Write Gameweeks\n",
    "\n",
    "Get gameweeks from fixtures table in silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98946da-80d7-4d9d-ba4d-bab0f7f3ccd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gameweek_df = fixtures_df.select(\n",
    "        \"season_key\", \n",
    "        \"gameweek_key\",\n",
    "        \"kickoff_time\",\n",
    "        \"gameweek\"\n",
    "    ).groupBy(\"gameweek_key\", \"season_key\", \"gameweek\"\n",
    "              ).agg(\n",
    "        F.min(\"kickoff_time\").alias(\"gameweek_start\"),\n",
    "        F.max(\"kickoff_time\").alias(\"gameweek_end\")\n",
    "    )\n",
    "\n",
    "write_to_table(\n",
    "    df = gameweek_df,\n",
    "    table_name = f\"{silver_schema}.gameweeks\",\n",
    "    mode = \"overwrite\",\n",
    "    merge_schema  = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tf_seasons_gameweeks_bronze_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}