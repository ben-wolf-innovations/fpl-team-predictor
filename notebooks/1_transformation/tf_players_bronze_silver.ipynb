{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e70f841a-f5c7-43f1-974b-77ceeb96425d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95124f92-00e5-4497-af2b-39533bf27a12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52de28e2-c517-4caf-871f-f0d54775f779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_fixture_key(df, \n",
    "                    season_col=\"season\", \n",
    "                    fixture_col=\"fixture\"):\n",
    "    return df.withColumn(\"fixture_key\",\n",
    "        F.concat(\n",
    "            F.lit(\"20\"), \n",
    "            F.regexp_replace(F.col(season_col), \"_\", \"\"), \n",
    "            F.lpad(F.col(fixture_col).cast(\"string\"), 3, \"0\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f83f8b-27c5-47eb-b976-fd379c8b7e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e6b4be-081b-42c4-a7cf-d54697de5e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b9b562-5bc1-4c3a-8859-79f25464cac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalise_season_keys(season_start: int) -> dict:\n",
    "    start_short = str(season_start)[-2:]\n",
    "    end_short = str(season_start + 1)[-2:]\n",
    "\n",
    "    return {\n",
    "        \"season_key\": f\"{season_start}{end_short}\",      # e.g. \"201617\"\n",
    "        \"season_table_suffix\": f\"{start_short}_{end_short}\",  # e.g. \"16_17\"\n",
    "        \"season_short\": f\"{start_short}{end_short}\"     # e.g. \"1617\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a7a26a-e250-4b9d-b38b-bbc80f59893e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_players(bronze_schema: str, \n",
    "                silver_schema: str, \n",
    "                seasons: list, \n",
    "                source_type: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Unified loader for player data from either 'HIST' (historic) or 'API' source.\n",
    "    Returns a DataFrame with player_id, player_key, season_key, player_season_key, player_season_spell_key,\n",
    "    first_name, second_name, team_key, initial_value, current_value,\n",
    "    first_fixture_key, last_fixture_key, position_key, scd_hash, player_surrogate_key,\n",
    "    effective_from, effective_to.\n",
    "    \"\"\"\n",
    "\n",
    "    fixtures_df = spark.table(f\"{silver_schema}.fixtures\"\n",
    "                    ).select(\n",
    "                        \"fixture_key\", \n",
    "                        \"home_team_key\", \n",
    "                        \"away_team_key\",\n",
    "                        \"gameweek_key\"\n",
    "                    )\n",
    "    player_records = []\n",
    "\n",
    "    for season in seasons:\n",
    "        season_key = \"20\" + season.replace(\"_\", \"\")\n",
    "\n",
    "        # Load player metadata\n",
    "        if source_type == \"HIST\":\n",
    "            player_meta = spark.table(f\"{bronze_schema}.players_raw_{season}\").select(\n",
    "                F.col(\"id\").alias(\"player_meta_id\"),\n",
    "                \"first_name\",\n",
    "                \"second_name\",\n",
    "                F.col(\"code\").alias(\"player_key\"),\n",
    "                F.col(\"element_type\").alias(\"position_key\")\n",
    "            ).filter(F.col(\"position_key\") != 5)  # Ignore managers from 24/25\n",
    "        elif source_type == \"API\":\n",
    "            player_meta = spark.table(f\"{bronze_schema}.elements_{season}\").select(\n",
    "                F.col(\"id\").alias(\"player_meta_id\"),\n",
    "                \"first_name\",\n",
    "                \"second_name\",\n",
    "                F.col(\"code\").alias(\"player_key\"),\n",
    "                F.col(\"element_type\").alias(\"position_key\")\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported source_type: {source_type}\")\n",
    "\n",
    "        # Load player stats\n",
    "        stats_df = spark.table(f\"{bronze_schema}.player_gameweek_stats_{season}\").select(\n",
    "            F.col(\"element\").alias(\"player_id\"),\n",
    "            \"was_home\",\n",
    "            \"fixture\",\n",
    "            \"round\",\n",
    "            \"value\"\n",
    "        ).withColumn(\"season\", F.lit(season))\n",
    "\n",
    "        stats_df = add_fixture_key(stats_df)\n",
    "\n",
    "        # Join player_key and position\n",
    "        stats_df = stats_df.join(\n",
    "            player_meta.select(\"player_meta_id\", \"player_key\", \"position_key\"),\n",
    "            stats_df[\"player_id\"] == player_meta[\"player_meta_id\"],\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # Join with fixtures to get team info\n",
    "        stats_df = stats_df.join(fixtures_df, on=\"fixture_key\", how=\"inner\").withColumn(\n",
    "            \"team_key\",\n",
    "            F.when(F.col(\"was_home\"), F.col(\"home_team_key\")).otherwise(F.col(\"away_team_key\"))\n",
    "        )\n",
    "\n",
    "        # Add season key\n",
    "        stats_df = stats_df.withColumn(\"season_key\", F.lit(season_key))\n",
    "\n",
    "        # Detect team changes using window\n",
    "        window_spec = Window.partitionBy(\"player_id\", \"season_key\").orderBy(\"gameweek_key\")\n",
    "        stats_df = stats_df.withColumn(\"prev_team_key\", F.lag(\"team_key\").over(window_spec))\n",
    "        stats_df = stats_df.withColumn(\n",
    "            \"team_change_flag\",\n",
    "            F.when(F.col(\"team_key\") != F.col(\"prev_team_key\"), 1).otherwise(0)\n",
    "        )\n",
    "        stats_df = stats_df.withColumn(\n",
    "            \"spell_group\",\n",
    "            F.sum(\"team_change_flag\").over(window_spec.rowsBetween(Window.unboundedPreceding, 0))\n",
    "        )\n",
    "\n",
    "        # Aggregate per spell\n",
    "        grouped_df = stats_df.groupBy(\n",
    "            \"player_key\", \"player_id\", \"season_key\", \"spell_group\", \"team_key\", \"position_key\"\n",
    "        ).agg(\n",
    "            F.first(\"value\").alias(\"initial_value\"),\n",
    "            F.last(\"value\").alias(\"current_value\"),\n",
    "            F.min(\"fixture_key\").alias(\"first_fixture_key\"),\n",
    "            F.max(\"fixture_key\").alias(\"last_fixture_key\"),\n",
    "            F.min(\"gameweek_key\").alias(\"first_gameweek_key\"),\n",
    "            F.max(\"gameweek_key\").alias(\"last_gameweek_key\")\n",
    "        )\n",
    "\n",
    "        # Generate unique spell key\n",
    "        grouped_df = grouped_df.withColumn(\n",
    "            \"player_season_spell_key\",\n",
    "            F.concat_ws(\"_\", F.col(\"season_key\"), F.col(\"player_id\").cast(\"string\"), F.col(\"spell_group\").cast(\"string\"))\n",
    "        ).withColumn(\n",
    "            \"player_season_key\",\n",
    "            F.concat(F.col(\"season_key\").cast(\"string\"), F.lpad(F.col(\"player_id\").cast(\"string\"), 3, \"0\")).cast(\"long\")\n",
    "        )\n",
    "\n",
    "        # SCD hash for team_key, player_season_spell_key, player_season_key\n",
    "        grouped_df = grouped_df.withColumn(\n",
    "            \"scd_hash\",\n",
    "            F.sha2(\n",
    "                F.concat_ws(\n",
    "                    \"||\",\n",
    "                    F.col(\"team_key\").cast(\"string\"),\n",
    "                    F.col(\"player_season_spell_key\").cast(\"string\"),\n",
    "                    F.col(\"player_season_key\").cast(\"string\")\n",
    "                ),\n",
    "                256\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Surrogate key: deterministic hash as long\n",
    "        grouped_df = grouped_df.withColumn(\n",
    "            \"player_surrogate_key\",\n",
    "            F.abs(\n",
    "                F.hash(\n",
    "                    F.concat_ws(\n",
    "                        \"||\",\n",
    "                        F.col(\"player_key\").cast(\"string\"),\n",
    "                        F.col(\"season_key\").cast(\"string\"),\n",
    "                        F.col(\"team_key\").cast(\"string\"),\n",
    "                        F.col(\"player_season_spell_key\").cast(\"string\"),\n",
    "                        F.col(\"player_season_key\").cast(\"string\")\n",
    "                    )\n",
    "                )\n",
    "            ).cast(\"long\")\n",
    "        ).withColumn(\n",
    "            \"team_season_key\",\n",
    "            F.concat(F.col(\"season_key\"), F.col(\"team_key\")).cast(\"int\")\n",
    "        )\n",
    "\n",
    "        # Join back metadata\n",
    "        final_df = grouped_df.join(\n",
    "            player_meta.select(\"player_key\", \"first_name\", \"second_name\"),\n",
    "            on=\"player_key\",\n",
    "            how=\"inner\"\n",
    "        ).select(\n",
    "            \"player_id\", \"player_key\", \"season_key\", \"player_season_key\", \"player_season_spell_key\",\n",
    "            \"first_name\", \"second_name\", \"team_key\", \"team_season_key\", \"position_key\",\n",
    "            \"initial_value\", \"current_value\",\n",
    "            F.col(\"first_fixture_key\").cast(\"int\"), \n",
    "            F.col(\"last_fixture_key\").cast(\"int\"),\n",
    "            F.col(\"first_gameweek_key\").cast(\"int\").alias(\"effective_from\"), \n",
    "            F.col(\"last_gameweek_key\").cast(\"int\").alias(\"effective_to\"),\n",
    "            \"scd_hash\",\n",
    "            \"player_surrogate_key\"\n",
    "        )\n",
    "\n",
    "        player_records.append(final_df)\n",
    "\n",
    "    #Union all seasons\n",
    "    silver_players_df = player_records[0]\n",
    "    for df in player_records[1:]:\n",
    "        silver_players_df = silver_players_df.unionByName(df)\n",
    "\n",
    "    return silver_players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23ae727-b691-410c-9335-f0c1d7eaf7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1d59f6-3ce8-48f5-92f8-b9ad429b4f8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"prod\"\n",
    "\n",
    "try:\n",
    "    PROTOCOL = dbutils.widgets.get(\"PROTOCOL\")\n",
    "except Exception:\n",
    "    PROTOCOL = \"HIST\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "valid_protocols = {\"HIST\", \"INCR\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "\n",
    "# Validate PROTOCOL\n",
    "if PROTOCOL not in valid_protocols:\n",
    "    print(f\"Invalid PROTOCOL: {PROTOCOL}. Must be one of {valid_protocols}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid PROTOCOL\")\n",
    "    \n",
    "bronze_schema = f\"fpl_bronze_{ENV}\"\n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "historic_seasons = [f\"{str(y)[2:]}_{str(y+1)[-2:]}\" for y in range(2016, 2025)]\n",
    "api_seasons = [\"25_26\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "596263c1-b0c1-4fcc-958f-c00d9edca106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Get Players and Write to Silver\n",
    "\n",
    "For historic seasons, player data is in players_raw_{season} and contains data such as name, element_type (position). \n",
    "\n",
    "For API seasons, this is in elements_{season}.\n",
    "\n",
    "players_gameweek_stats_{season} contains weekly stats per player. Using this and fixtures, can derive the team of the player each week, as well as FPL value (price)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882b0e47-12ab-4f3f-9164-c16aebd69f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if PROTOCOL == \"HIST\":\n",
    "    df_historic = get_players(bronze_schema, silver_schema, historic_seasons, source_type=\"HIST\")\n",
    "    df_api = get_players(bronze_schema, silver_schema, api_seasons, source_type=\"API\")\n",
    "    players_df = df_historic.unionByName(df_api)\n",
    "\n",
    "    write_to_table(\n",
    "        df = players_df,\n",
    "        table_name = f\"{silver_schema}.players\",\n",
    "        mode =  \"overwrite\",\n",
    "        merge_schema = False\n",
    "    ) \n",
    "\n",
    "elif PROTOCOL == \"INCR\":\n",
    "    players_df = get_players(bronze_schema, silver_schema, api_seasons, source_type=\"API\")\n",
    "\n",
    "    merge_to_table(\n",
    "        df = players_df,\n",
    "        table_name = f\"{silver_schema}.players\",\n",
    "        merge_condition = \"target.player_surrogate_key = source.player_surrogate_key\",\n",
    "        spark = spark\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tf_players_bronze_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}