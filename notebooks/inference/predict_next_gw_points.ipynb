{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5ae9bb-6a59-453c-8e27-2a985b799ff3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1d0c2b-c344-457b-8f60-a011463f33a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Player Features function"
    }
   },
   "outputs": [],
   "source": [
    "def build_player_features(\n",
    "    gameweek_stats_df,\n",
    "    rolling_window_size,\n",
    "    silver_schema,\n",
    "    training = True\n",
    "):\n",
    "    player_base_window = Window.partitionBy(\"player_key\", \"season_key\", \"team_key\").orderBy(\"gameweek_key\")\n",
    "\n",
    "    if training:\n",
    "        player_rolling_window = player_base_window.rowsBetween(-rolling_window_size, -1)\n",
    "    else:\n",
    "        player_rolling_window = player_base_window.rowsBetween(-rolling_window_size + 1, 0)\n",
    "\n",
    "    player_first_gw =  F.row_number().over(player_base_window) == 1\n",
    "\n",
    "    # Calculate rolling player stats\n",
    "    player_rolling_exprs = {\n",
    "        \"rolling_expected_goals\": F.when(player_first_gw, None).otherwise(F.sum(\"expected_goals\").over(player_rolling_window)),\n",
    "        \"rolling_expected_assists\": F.when(player_first_gw, None).otherwise(F.sum(\"expected_assists\").over(player_rolling_window)),\n",
    "        \"rolling_expected_goal_involvements\": F.when(player_first_gw, None).otherwise(F.sum(\"expected_goal_involvements\").over(player_rolling_window)),\n",
    "        \"rolling_goals_scored\": F.when(player_first_gw, None).otherwise(F.sum(\"goals_scored\").over(player_rolling_window)),\n",
    "        \"rolling_assists\": F.when(player_first_gw, None).otherwise(F.sum(\"assists\").over(player_rolling_window)),\n",
    "        \"rolling_total_points\": F.when(player_first_gw, None).otherwise(F.sum(\"total_points\").over(player_rolling_window)),\n",
    "        \"rolling_minutes\": F.when(player_first_gw, None).otherwise(F.sum(\"minutes\").over(player_rolling_window)),\n",
    "        \"rolling_clean_sheets\": F.when(player_first_gw, None).otherwise(F.sum(\"clean_sheets\").over(player_rolling_window)),\n",
    "        \"rolling_bps\": F.when(player_first_gw, None).otherwise(F.sum(\"bps\").over(player_rolling_window)),\n",
    "        \"rolling_ict_index\": F.when(player_first_gw, None).otherwise(F.sum(\"ict_index\").over(player_rolling_window)),\n",
    "        \"rolling_influence\": F.when(player_first_gw, None).otherwise(F.sum(\"influence\").over(player_rolling_window)),\n",
    "        \"rolling_creativity\": F.when(player_first_gw, None).otherwise(F.sum(\"creativity\").over(player_rolling_window)),\n",
    "        \"rolling_threat\": F.when(player_first_gw, None).otherwise(F.sum(\"threat\").over(player_rolling_window)),\n",
    "        \"rolling_defensive_contribution\": F.when(player_first_gw, None).otherwise(F.sum(\"defensive_contribution\").over(player_rolling_window)),\n",
    "        \"rolling_clearances_blocks_interceptions\": F.when(player_first_gw, None).otherwise(F.sum(\"clearances_blocks_interceptions\").over(player_rolling_window)),\n",
    "        \"rolling_bonus\": F.when(player_first_gw, None).otherwise(F.sum(\"bonus\").over(player_rolling_window)),\n",
    "        # Position-specific: saves only for GK\n",
    "        \"rolling_saves\": F.when(player_first_gw, None).otherwise(F.sum(F.when(F.col(\"position_key\") == 1, F.col(\"saves\")).otherwise(None)).over(player_rolling_window)),\n",
    "        \"rolling_games_played\": F.when(player_first_gw, None).otherwise(F.count(\"fixture_key\").over(player_rolling_window)),\n",
    "        # Rolling FPL points\n",
    "        \"rolling_minutes_points\": F.when(player_first_gw, None).otherwise(F.sum(\"minutes_points\").over(player_rolling_window)),\n",
    "        \"rolling_assist_points\": F.when(player_first_gw, None).otherwise(F.sum(\"assist_points\").over(player_rolling_window)),\n",
    "        \"rolling_goal_points\": F.when(player_first_gw, None).otherwise(F.sum(\"goal_points\").over(player_rolling_window)),\n",
    "        \"rolling_clean_sheet_points\": F.when(player_first_gw, None).otherwise(F.sum(\"clean_sheet_points\").over(player_rolling_window)),\n",
    "        \"rolling_defensive_contribution_points\": F.when(player_first_gw, None).otherwise(F.sum(\"defensive_contribution_points\").over(player_rolling_window)),\n",
    "        \"rolling_penalty_miss_points\": F.when(player_first_gw, None).otherwise(F.sum(\"penalty_miss_points\").over(player_rolling_window)),\n",
    "        \"rolling_goals_conceded_points\": F.when(player_first_gw, None).otherwise(F.sum(\"goals_conceded_points\").over(player_rolling_window)),\n",
    "        \"rolling_yellow_card_points\": F.when(player_first_gw, None).otherwise(F.sum(\"yellow_card_points\").over(player_rolling_window)),\n",
    "        \"rolling_red_card_points\": F.when(player_first_gw, None).otherwise(F.sum(\"red_card_points\").over(player_rolling_window)),\n",
    "        \"rolling_own_goal_points\": F.when(player_first_gw, None).otherwise(F.sum(\"own_goal_points\").over(player_rolling_window))\n",
    "    }\n",
    "\n",
    "    player_features_df = gameweek_stats_df.withColumns(player_rolling_exprs)\n",
    "\n",
    "    # Calculate rolling averages to 3 decimal places, set to null for first gameweek\n",
    "    player_avg_exprs = {\n",
    "        f\"avg_{k[8:]}\": F.when(\n",
    "            player_first_gw,\n",
    "            None\n",
    "        ).otherwise(\n",
    "            F.round(\n",
    "                F.when(\n",
    "                    F.col(\"rolling_games_played\") != 0,\n",
    "                    F.col(k) / F.col(\"rolling_games_played\")\n",
    "                ).otherwise(None),\n",
    "                3\n",
    "            )\n",
    "        )\n",
    "        for k in player_rolling_exprs if k != \"rolling_games_played\"\n",
    "    }\n",
    "    player_features_df = player_features_df.withColumns(player_avg_exprs)\n",
    "\n",
    "    # Join with team_features for contextual strength and match_points\n",
    "    pf = player_features_df.alias(\"pf\")\n",
    "\n",
    "    team_features_df = build_team_features(\n",
    "        gameweek_stats_df = gameweek_stats_df,\n",
    "        rolling_window_size = rolling_window_size,\n",
    "        silver_schema = silver_schema,\n",
    "        training = True\n",
    "    )\n",
    "\n",
    "    tf = team_features_df.alias(\"tf\")\n",
    "\n",
    "    team_strength_cols = [\n",
    "        F.col(\"team_key\").alias(\"_team_key\"), F.col(\"season_key\").alias(\"team_season_key\"), F.col(\"gameweek_key\").alias(\"team_gamweek_key\"),\n",
    "        \"rolling_points\", \"rolling_team_expected_goals\",\n",
    "        \"rolling_expected_goals_against\", \"rolling_goal_difference\",\n",
    "        \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\",\n",
    "        \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\",\n",
    "        \"avg_goal_difference\", \"match_points\"\n",
    "    ]\n",
    "\n",
    "    tf = tf.select(*team_strength_cols)\n",
    "\n",
    "    pf = pf.join(\n",
    "        tf,\n",
    "        (pf[\"team_key\"] == tf[\"_team_key\"]) &\n",
    "        (pf[\"season_key\"] == tf[\"team_season_key\"]) &\n",
    "        (pf[\"gameweek_key\"] == tf[\"team_gamweek_key\"]),\n",
    "        how=\"left\"\n",
    "    ).drop(\"_team_key\", \"team_season_key\", \"team_gamweek_key\")\n",
    "\n",
    "    # Join opponent team strength features\n",
    "    tf_opp = team_features_df.alias(\"tf_opp\")\n",
    "    opponent_team_strength_cols = [\n",
    "        F.col(\"team_key\").alias(\"_opponent_team_key\"),\n",
    "        F.col(\"season_key\").alias(\"opponent_season_key\"),\n",
    "        F.col(\"gameweek_key\").alias(\"opponent_gameweek_key\"),\n",
    "        F.col(\"rolling_points\").alias(\"opponent_rolling_points\"),\n",
    "        F.col(\"rolling_team_expected_goals\").alias(\"opponent_rolling_team_expected_goals\"),\n",
    "        F.col(\"rolling_expected_goals_against\").alias(\"opponent_rolling_expected_goals_against\"),\n",
    "        F.col(\"rolling_goal_difference\").alias(\"opponent_rolling_goal_difference\"),\n",
    "        F.col(\"avg_team_expected_goals\").alias(\"opponent_avg_team_expected_goals\"),\n",
    "        F.col(\"avg_team_expected_assists\").alias(\"opponent_avg_team_expected_assists\"),\n",
    "        F.col(\"avg_team_expected_goal_involvements\").alias(\"opponent_avg_team_expected_goal_involvements\"),\n",
    "        F.col(\"avg_expected_goals_against\").alias(\"opponent_avg_expected_goals_against\"),\n",
    "        F.col(\"avg_expected_assists_against\").alias(\"opponent_avg_expected_assists_against\"),\n",
    "        F.col(\"avg_expected_goal_involvements_against\").alias(\"opponent_avg_expected_goal_involvements_against\"),\n",
    "        F.col(\"avg_goal_difference\").alias(\"opponent_avg_goal_difference\")\n",
    "    ]\n",
    "\n",
    "    tf_opp = team_features_df.select(*opponent_team_strength_cols)\n",
    "\n",
    "    pf = pf.join(\n",
    "        tf_opp,\n",
    "        (pf[\"opponent_team_key\"] == tf_opp[\"_opponent_team_key\"]) &\n",
    "        (pf[\"season_key\"] == tf_opp[\"opponent_season_key\"]) &\n",
    "        (pf[\"gameweek_key\"] == tf_opp[\"opponent_gameweek_key\"]),\n",
    "        how=\"left\"\n",
    "    ).drop(\"_opponent_team_key\", \"opponent_season_key\", \"opponent_gameweek_key\")\n",
    "\n",
    "    # For GK/DEF/MID, add team defensive rolling stats\n",
    "    player_features_df = pf.withColumns({\n",
    "        \"team_rolling_goals_conceded\": F.when(F.col(\"position_key\").isin([1,2,3]), F.col(\"rolling_expected_goals_against\")).otherwise(None),\n",
    "        \"team_rolling_goal_difference\": F.when(F.col(\"position_key\").isin([1,2,3]), F.col(\"rolling_goal_difference\")).otherwise(None)\n",
    "    })\n",
    "\n",
    "    # Add ratios of team share\n",
    "    player_features_df = player_features_df.withColumns({\n",
    "        \"player_share_of_team_xG\": F.round(\n",
    "            F.when(\n",
    "                F.col(\"rolling_team_expected_goals\") != 0,\n",
    "                F.col(\"rolling_expected_goals\") / F.col(\"rolling_team_expected_goals\")\n",
    "            ).otherwise(None),\n",
    "            3\n",
    "        ),\n",
    "        \"player_share_of_team_points\": F.round(\n",
    "            F.when(\n",
    "                F.col(\"rolling_points\") != 0,\n",
    "                F.col(\"rolling_total_points\") / F.col(\"rolling_points\")\n",
    "            ).otherwise(None),\n",
    "            3\n",
    "        )\n",
    "    })\n",
    "\n",
    "    # Select final columns, including raw stats from stats_df and match_points from team_features\n",
    "    raw_stats_cols = [\n",
    "        \"fixture_key\", \"player_id\", \"player_key\", \"player_season_key\", \"player_fixture_key\", \"team_key\", \"season_key\", \"gameweek_key\", \"position_key\", \"opponent_team_key\", \"was_home\", \"exp_stats_available\", \"def_con_available\", \"total_points\"\n",
    "    ]\n",
    "\n",
    "    final_cols = [\n",
    "        *raw_stats_cols,\n",
    "        # Rolling stats\n",
    "        *list(player_rolling_exprs.keys()),\n",
    "        # Rolling averages\n",
    "        *list(player_avg_exprs.keys()),\n",
    "        # Team/contextual features\n",
    "        \"rolling_points\", \"rolling_team_expected_goals\", \"rolling_expected_goals_against\", \"rolling_goal_difference\",\n",
    "        \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\",\n",
    "        \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\",\n",
    "        \"avg_goal_difference\",\n",
    "        \"match_points\",\n",
    "        \"team_rolling_goals_conceded\", \"team_rolling_goal_difference\",\n",
    "        \"player_share_of_team_xG\", \"player_share_of_team_points\",\n",
    "        # Opponent team strength features\n",
    "        \"opponent_rolling_points\", \"opponent_rolling_team_expected_goals\",\n",
    "        \"opponent_rolling_expected_goals_against\", \"opponent_rolling_goal_difference\",\n",
    "        \"opponent_avg_team_expected_goals\", \"opponent_avg_team_expected_assists\", \"opponent_avg_team_expected_goal_involvements\",\n",
    "        \"opponent_avg_expected_goals_against\", \"opponent_avg_expected_assists_against\", \"opponent_avg_expected_goal_involvements_against\",\n",
    "        \"opponent_avg_goal_difference\"\n",
    "    ]\n",
    "\n",
    "    player_features_df = player_features_df.select(*final_cols)\n",
    "\n",
    "    player_features_df = player_features_df.withColumns({\n",
    "        \"was_home\": F.col(\"was_home\").cast(\"int\"),\n",
    "        \"exp_stats_available\": F.col(\"exp_stats_available\").cast(\"int\"),\n",
    "        \"def_con_available\": F.col(\"def_con_available\").cast(\"int\")\n",
    "    })\n",
    "\n",
    "    return player_features_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32d7c94-a8a7-401d-b33d-d29f8b6f48bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Team Features function"
    }
   },
   "outputs": [],
   "source": [
    "def build_team_features(\n",
    "    gameweek_stats_df,\n",
    "    rolling_window_size,\n",
    "    silver_schema,\n",
    "    training = True\n",
    "):    \n",
    "        \n",
    "    fixtures_df = spark.read.table(f\"{silver_schema}.fixtures\").filter(F.col(\"home_team_score\").isNotNull())\n",
    "\n",
    "    teams_df = spark.read.table(f\"{silver_schema}.teams\")\n",
    "\n",
    "    #Aggregate xG, xA, and exp_stats_available per team per fixture\n",
    "    team_xg_xa_df = gameweek_stats_df.groupBy(\n",
    "        \"fixture_key\", \"team_key\"\n",
    "        ).agg(\n",
    "            F.sum(\"expected_goals\").alias(\"team_expected_goals\"),\n",
    "            F.sum(\"expected_assists\").alias(\"team_expected_assists\"),\n",
    "            F.max(\"exp_stats_available\").alias(\"team_exp_stats_available\")\n",
    "        ).withColumn(\n",
    "            \"team_expected_goal_involvements\",\n",
    "            F.col(\"team_expected_goals\") + F.col(\"team_expected_assists\")\n",
    "        )\n",
    "\n",
    "    #Create opponent xG/xA aggregates\n",
    "    opponent_xg_xa_df = team_xg_xa_df.select(\n",
    "            \"fixture_key\",\n",
    "            F.col(\"team_key\").alias(\"opponent_team_key\"),\n",
    "            F.col(\"team_expected_goals\").alias(\"expected_goals_against\"),\n",
    "            F.col(\"team_expected_assists\").alias(\"expected_assists_against\"),\n",
    "            F.col(\"team_expected_goal_involvements\").alias(\"expected_goal_involvements_against\")\n",
    "        )\n",
    "\n",
    "    #Transform fixtures into team-level records\n",
    "    home_df = fixtures_df.select(\n",
    "            \"fixture_key\",\n",
    "            \"season_key\",\n",
    "            \"gameweek_key\",\n",
    "            F.col(\"home_team_key\").alias(\"team_key\"),\n",
    "            F.col(\"away_team_key\").alias(\"opponent_team_key\"),\n",
    "            F.lit(True).alias(\"was_home\"),\n",
    "            F.col(\"home_team_score\").alias(\"goals_for\"),\n",
    "            F.col(\"away_team_score\").alias(\"goals_against\")\n",
    "        )\n",
    "\n",
    "    away_df = fixtures_df.select(\n",
    "            \"fixture_key\",\n",
    "            \"season_key\",\n",
    "            \"gameweek_key\",\n",
    "            F.col(\"away_team_key\").alias(\"team_key\"),\n",
    "            F.col(\"home_team_key\").alias(\"opponent_team_key\"),\n",
    "            F.lit(False).alias(\"was_home\"),\n",
    "            F.col(\"away_team_score\").alias(\"goals_for\"),\n",
    "            F.col(\"home_team_score\").alias(\"goals_against\")\n",
    "        )\n",
    "\n",
    "    team_fixtures_df = home_df.unionByName(away_df)\n",
    "\n",
    "    # Window specs for rolling metrics up to previous GW\n",
    "    base_window = Window.partitionBy(\"team_key\", \"season_key\").orderBy(\"gameweek_key\")\n",
    "\n",
    "    if training:\n",
    "        rolling_window = base_window.rowsBetween(-rolling_window_size, -1)\n",
    "    else:\n",
    "        rolling_window = base_window.rowsBetween(-rolling_window_size + 1, 0)\n",
    "\n",
    "    first_gw = F.row_number().over(base_window) == 1\n",
    "    \n",
    "    #Add match-level metrics\n",
    "    team_fixtures_df = team_fixtures_df.withColumns({\n",
    "        \"goal_diff\": F.col(\"goals_for\") - F.col(\"goals_against\"),\n",
    "        \"match_points\": F.when(F.col(\"goals_for\") > F.col(\"goals_against\"), F.lit(3))\n",
    "                        .when(F.col(\"goals_for\") == F.col(\"goals_against\"), F.lit(1))\n",
    "                        .otherwise(F.lit(0))\n",
    "    })\n",
    "\n",
    "    #Join team xG/xA and opponent xG/xA\n",
    "    team_fixtures_df = team_fixtures_df.join(\n",
    "        team_xg_xa_df, \n",
    "        on=[\"fixture_key\", \"team_key\"], \n",
    "        how=\"left\"\n",
    "        ).join(\n",
    "            opponent_xg_xa_df, \n",
    "            on=[\"fixture_key\", \"opponent_team_key\"], \n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # Rolling metrics expressions\n",
    "    rolling_exprs = {\n",
    "        \"rolling_points\": F.when(first_gw, None).otherwise(F.sum(\"match_points\").over(rolling_window)),\n",
    "        \"home_rolling_points\": F.when(first_gw, None).otherwise(F.sum(F.when(F.col(\"was_home\"), F.col(\"match_points\")).otherwise(0)).over(rolling_window)),\n",
    "        \"away_rolling_points\": F.when(first_gw, None).otherwise(F.sum(F.when(~F.col(\"was_home\"), F.col(\"match_points\")).otherwise(0)).over(rolling_window)),\n",
    "        \"rolling_team_expected_goals\": F.when(first_gw, None).otherwise(F.sum(\"team_expected_goals\").over(rolling_window)),\n",
    "        \"rolling_team_expected_assists\": F.when(first_gw, None).otherwise(F.sum(\"team_expected_assists\").over(rolling_window)),\n",
    "        \"rolling_team_expected_goal_involvements\": F.when(first_gw, None).otherwise(F.sum(\"team_expected_goal_involvements\").over(rolling_window)),\n",
    "        \"rolling_expected_goals_against\": F.when(first_gw, None).otherwise(F.sum(\"expected_goals_against\").over(rolling_window)),\n",
    "        \"rolling_expected_assists_against\": F.when(first_gw, None).otherwise(F.sum(\"expected_assists_against\").over(rolling_window)),\n",
    "        \"rolling_expected_goal_involvements_against\": F.when(first_gw, None).otherwise(F.sum(\"expected_goal_involvements_against\").over(rolling_window)),\n",
    "        \"rolling_goal_difference\": F.when(first_gw, None).otherwise(F.sum(\"goal_diff\").over(rolling_window)),\n",
    "        \"rolling_games_played\": F.when(first_gw, None).otherwise(F.count(\"fixture_key\").over(rolling_window)),\n",
    "        \"avg_team_expected_goals\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_team_expected_goals\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_team_expected_assists\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_team_expected_assists\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_team_expected_goal_involvements\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_team_expected_goal_involvements\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_expected_goals_against\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_expected_goals_against\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_expected_assists_against\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_expected_assists_against\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_expected_goal_involvements_against\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_expected_goal_involvements_against\") / F.col(\"rolling_games_played\"), 3)),\n",
    "        \"avg_goal_difference\": F.when(first_gw, None).otherwise(F.round(F.col(\"rolling_goal_difference\") / F.col(\"rolling_games_played\"), 3))\n",
    "    }\n",
    "\n",
    "    team_fixtures_df = team_fixtures_df.withColumns(rolling_exprs)\n",
    "\n",
    "    #Join team metadata\n",
    "    team_features_df = team_fixtures_df.join(\n",
    "        teams_df.select(\"team_key\", \"team_name\", \"team_name_short\", \"is_promoted\", \"is_relegated\", \"season_key\"),\n",
    "        on=[\"team_key\", \"season_key\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    #Select final columns\n",
    "    team_features_df = team_features_df.select(\n",
    "        \"team_key\", \"team_name\", \"team_name_short\", \"season_key\", \"gameweek_key\", \"fixture_key\",\n",
    "        \"was_home\", \"goals_for\", \"goals_against\", \"goal_diff\", \"match_points\",\n",
    "        \"team_expected_goals\", \"team_expected_assists\", \"team_expected_goal_involvements\",\n",
    "        \"expected_goals_against\", \"expected_assists_against\", \"expected_goal_involvements_against\",\n",
    "        \"team_exp_stats_available\",\n",
    "        \"rolling_points\", \"home_rolling_points\", \"away_rolling_points\",\n",
    "        \"rolling_team_expected_goals\", \"rolling_team_expected_assists\", \"rolling_team_expected_goal_involvements\",\n",
    "        \"rolling_expected_goals_against\", \"rolling_expected_assists_against\", \"rolling_expected_goal_involvements_against\",\n",
    "        \"rolling_goal_difference\", \"rolling_games_played\",\n",
    "        \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\",\n",
    "        \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\",\n",
    "        \"avg_goal_difference\",\n",
    "        \"is_promoted\", \"is_relegated\"\n",
    "    )\n",
    "\n",
    "    return team_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b5fc3-ba60-41ac-ac31-23bbdcfa915d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Variables"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "    \n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "feature_schema = f\"fpl_feature_{ENV}\"\n",
    "\n",
    "rolling_window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216e894c-d86b-479b-b481-2769e40248e7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in all tables"
    }
   },
   "outputs": [],
   "source": [
    "fixtures_df = spark.read.table(f\"{silver_schema}.fixtures\")\n",
    "team_features_df = spark.read.table(f\"{feature_schema}.team_features\")\n",
    "gameweek_stats_df = spark.read.table(f\"{silver_schema}.gameweek_stats\")\n",
    "players_df = spark.read.table(f\"{silver_schema}.players\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbdc3f9d-8d3d-4526-8085-3c6b0568ae33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get max fixture in features and next week fixtures"
    }
   },
   "outputs": [],
   "source": [
    "max_gameweek_row = team_features_df.agg(F.max(\"gameweek_key\").alias(\"max_gameweek_key\")).collect()[0]\n",
    "max_gameweek = max_gameweek_row[\"max_gameweek_key\"]\n",
    "next_gameweek_key = max_gameweek + 1\n",
    "\n",
    "next_fixtures_df = fixtures_df.filter(\n",
    "    F.col(\"gameweek_key\") == next_gameweek_key\n",
    ").select(\n",
    "    \"fixture_key\", \n",
    "    \"home_team_key\", \n",
    "    \"away_team_key\", \n",
    "    \"gameweek_key\", \n",
    "    \"season_key\"\n",
    ")\n",
    "\n",
    "display(next_fixtures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3986de-e208-4bc9-95c8-7955dc4b4cc7",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763817338380}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Get players for next fixtures"
    }
   },
   "outputs": [],
   "source": [
    "teams_next_fixtures_df = next_fixtures_df.select(\n",
    "    \"fixture_key\", \n",
    "    \"gameweek_key\", \n",
    "    \"season_key\",\n",
    "    F.col(\"home_team_key\").alias(\"team_key\"),\n",
    "    F.col(\"away_team_key\").alias(\"opponent_team_key\"),\n",
    "    F.lit(1).alias(\"was_home\")\n",
    ").unionByName(\n",
    "    next_fixtures_df.select(\n",
    "        \"fixture_key\", \n",
    "        \"gameweek_key\", \n",
    "        \"season_key\",\n",
    "        F.col(\"away_team_key\").alias(\"team_key\"),\n",
    "        F.col(\"home_team_key\").alias(\"opponent_team_key\"),\n",
    "        F.lit(0).alias(\"was_home\")\n",
    "    )\n",
    ")\n",
    "\n",
    "players_next_fixtures_df = teams_next_fixtures_df.alias(\"tf\").join(\n",
    "    players_df.alias(\"pf\"),\n",
    "    (F.col(\"tf.team_key\") == F.col(\"pf.team_key\")) &\n",
    "    (F.col(\"pf.last_gameweek_key\") == max_gameweek),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"tf.fixture_key\"),\n",
    "    F.col(\"tf.team_key\"),\n",
    "    F.col(\"tf.opponent_team_key\"),\n",
    "    F.col(\"tf.season_key\"),\n",
    "    F.col(\"tf.gameweek_key\"),\n",
    "    F.col(\"pf.player_key\"),\n",
    "    F.col(\"tf.was_home\").cast(\"int\").alias(\"was_home\")\n",
    ")\n",
    "\n",
    "display(players_next_fixtures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8c18c2-c20d-4d63-a82c-f8a70cd31429",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build Player Features for current gameweek"
    }
   },
   "outputs": [],
   "source": [
    "current_season_key = gameweek_stats_df.agg(F.max(\"season_key\").alias(\"max_season_key\")).collect()[0][\"max_season_key\"]\n",
    "stats_df = gameweek_stats_df.filter(F.col(\"season_key\") == current_season_key)\n",
    "\n",
    "player_features_df = build_player_features(\n",
    "    gameweek_stats_df = stats_df,\n",
    "    rolling_window_size = rolling_window_size,\n",
    "    silver_schema = silver_schema,\n",
    "    training = False\n",
    ")\n",
    "\n",
    "#get rolling stats per player\n",
    "window_spec = Window.partitionBy(\"player_key\").orderBy(F.col(\"gameweek_key\").desc())\n",
    "player_features_max_df = player_features_df.withColumn(\n",
    "    \"rn\", \n",
    "    F.row_number().over(window_spec)\n",
    "    ).filter(\n",
    "        F.col(\"rn\") == 1\n",
    "    ).drop(\"rn\")\n",
    "\n",
    "feature_prefixes = (\"rolling\", \"avg\")\n",
    "feature_cols = [\n",
    "    c for c in player_features_max_df.columns\n",
    "    if c.startswith(feature_prefixes)\n",
    "]\n",
    "\n",
    "player_current_features_df = player_features_max_df.select(\n",
    "    \"player_key\",\n",
    "    \"position_key\",\n",
    "    \"exp_stats_available\",\n",
    "    \"def_con_available\",\n",
    "    *feature_cols\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a224e8a4-43f5-4340-856f-4e3d0278289e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get opponent team features for current gameweek"
    }
   },
   "outputs": [],
   "source": [
    "team_features_df = build_team_features(\n",
    "    gameweek_stats_df = stats_df,\n",
    "    rolling_window_size = rolling_window_size,\n",
    "    silver_schema = silver_schema,\n",
    "    training = False\n",
    ")\n",
    "\n",
    "#get rolling stats per player\n",
    "window_spec = Window.partitionBy(\"team_key\").orderBy(F.col(\"gameweek_key\").desc())\n",
    "team_features_max_df = team_features_df.withColumn(\n",
    "    \"rn\", \n",
    "    F.row_number().over(window_spec)\n",
    "    ).filter(\n",
    "        F.col(\"rn\") == 1\n",
    "    ).drop(\"rn\")\n",
    "\n",
    "opponent_team_features_df = team_features_max_df.select(\n",
    "    F.col(\"team_key\").alias(\"opponent_team_key\"),\n",
    "    F.col(\"rolling_points\").alias(\"opponent_rolling_points\"),\n",
    "    F.col(\"rolling_team_expected_goals\").alias(\"opponent_rolling_team_expected_goals\"),\n",
    "    F.col(\"rolling_expected_goals_against\").alias(\"opponent_rolling_expected_goals_against\"),\n",
    "    F.col(\"rolling_goal_difference\").alias(\"opponent_rolling_goal_difference\"),\n",
    "    F.col(\"avg_team_expected_goals\").alias(\"opponent_avg_team_expected_goals\"),\n",
    "    F.col(\"avg_team_expected_assists\").alias(\"opponent_avg_team_expected_assists\"),\n",
    "    F.col(\"avg_team_expected_goal_involvements\").alias(\"opponent_avg_team_expected_goal_involvements\"),\n",
    "    F.col(\"avg_expected_goals_against\").alias(\"opponent_avg_expected_goals_against\"),\n",
    "    F.col(\"avg_expected_assists_against\").alias(\"opponent_avg_expected_assists_against\"),\n",
    "    F.col(\"avg_expected_goal_involvements_against\").alias(\"opponent_avg_expected_goal_involvements_against\"),\n",
    "    F.col(\"avg_goal_difference\").alias(\"opponent_avg_goal_difference\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732f6a75-0101-477e-8eef-4a14b9be20c8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763822130868}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "player_inference_features_df = players_next_fixtures_df.join(\n",
    "    player_current_features_df,\n",
    "    on=\"player_key\",\n",
    "    how=\"inner\"\n",
    ").join(\n",
    "    opponent_team_features_df,\n",
    "    on=\"opponent_team_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "display(player_inference_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28ce669-c585-4c3d-a197-484f1472b27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7899776272513194>, line 30\u001B[0m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Predict total_points\u001B[39;00m\n",
       "\u001B[1;32m     28\u001B[0m predictions_df \u001B[38;5;241m=\u001B[39m rf_model\u001B[38;5;241m.\u001B[39mtransform(player_inference_df)\n",
       "\u001B[0;32m---> 30\u001B[0m display(predictions_df)\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Convert to Pandas and add prediction column\u001B[39;00m\n",
       "\u001B[1;32m     33\u001B[0m inference_pdf \u001B[38;5;241m=\u001B[39m player_inference_df\u001B[38;5;241m.\u001B[39mtoPandas()\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:48\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n",
       "\u001B[0;32m---> 48\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n",
       "\u001B[1;32m     49\u001B[0m     ip_display({\n",
       "\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n",
       "\u001B[1;32m     51\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     52\u001B[0m     },\n",
       "\u001B[1;32m     53\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:134\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdbruntime\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdriver_error_classes_pb2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DriverErrorCode\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:103\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    101\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n",
       "\u001B[1;32m    102\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n",
       "\u001B[0;32m--> 103\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n",
       "\u001B[1;32m    104\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n",
       "\u001B[1;32m    105\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    106\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n",
       "\u001B[1;32m    107\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n",
       "\u001B[1;32m    108\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n",
       "\u001B[1;32m    110\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1882\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n",
       "\u001B[1;32m   1861\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1862\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n",
       "\u001B[1;32m   1863\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1879\u001B[0m \n",
       "\u001B[1;32m   1880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1881\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1882\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n",
       "\u001B[1;32m   1883\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1884\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1308\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n",
       "\u001B[1;32m   1305\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1308\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n",
       "\u001B[1;32m   1309\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1310\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: [FAILED_EXECUTE_UDF] User defined function (`RandomForestRegressionModel$$Lambda$25617/0x000000f005203138`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double) failed due to: java.lang.ArrayIndexOutOfBoundsException. SQLSTATE: 39000"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "[FAILED_EXECUTE_UDF] User defined function (`RandomForestRegressionModel$$Lambda$25617/0x000000f005203138`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double) failed due to: java.lang.ArrayIndexOutOfBoundsException. SQLSTATE: 39000"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "FAILED_EXECUTE_UDF",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "39000",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-7899776272513194>, line 30\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Predict total_points\u001B[39;00m\n\u001B[1;32m     28\u001B[0m predictions_df \u001B[38;5;241m=\u001B[39m rf_model\u001B[38;5;241m.\u001B[39mtransform(player_inference_df)\n\u001B[0;32m---> 30\u001B[0m display(predictions_df)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Convert to Pandas and add prediction column\u001B[39;00m\n\u001B[1;32m     33\u001B[0m inference_pdf \u001B[38;5;241m=\u001B[39m player_inference_df\u001B[38;5;241m.\u001B[39mtoPandas()\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:133\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cf_helper \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:97\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m     95\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:48\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n\u001B[0;32m---> 48\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n\u001B[1;32m     49\u001B[0m     ip_display({\n\u001B[1;32m     50\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     52\u001B[0m     },\n\u001B[1;32m     53\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:134\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdbruntime\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproto\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdriver_error_classes_pb2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DriverErrorCode\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:103\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rowLimit \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    101\u001B[0m     connectDataFrame \u001B[38;5;241m=\u001B[39m cast(ConnectDataFrame, connectDataFrame\u001B[38;5;241m.\u001B[39mlimit(rowLimit \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    102\u001B[0m results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m--> 103\u001B[0m                List[\u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n\u001B[1;32m    104\u001B[0m                    \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n\u001B[1;32m    105\u001B[0m                    compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    106\u001B[0m                    row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n\u001B[1;32m    107\u001B[0m                    byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n\u001B[1;32m    108\u001B[0m pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    110\u001B[0m schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(pyspark_struct)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1882\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n\u001B[1;32m   1861\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1862\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n\u001B[1;32m   1863\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1879\u001B[0m \n\u001B[1;32m   1880\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1881\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1882\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n\u001B[1;32m   1883\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1884\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1308\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n\u001B[1;32m   1305\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1308\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n\u001B[1;32m   1309\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1310\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: [FAILED_EXECUTE_UDF] User defined function (`RandomForestRegressionModel$$Lambda$25617/0x000000f005203138`: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double) failed due to: java.lang.ArrayIndexOutOfBoundsException. SQLSTATE: 39000"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "model_name = \"FPL_TotalPoints_RandomForest\"\n",
    "model_alias = \"production\"\n",
    "model_uri = f\"models:/{model_name}@{model_alias}\"\n",
    "uc_volume_path = \"/Volumes/workspace/fpl_feature_dev/mlops\"\n",
    "\n",
    "# List of features used in training (from player_features_df)\n",
    "feature_cols = [\n",
    "    \"was_home\", \"rolling_expected_goals\", \"rolling_expected_assists\", \"rolling_expected_goal_involvements\", \"rolling_goals_scored\", \"rolling_assists\", \"rolling_total_points\", \"rolling_minutes\", \"rolling_clean_sheets\", \"rolling_bps\", \"rolling_ict_index\", \"rolling_influence\", \"rolling_creativity\", \"rolling_threat\", \"rolling_defensive_contribution\", \"rolling_clearances_blocks_interceptions\", \"rolling_bonus\", \"rolling_saves\", \"rolling_games_played\", \"rolling_minutes_points\", \"rolling_assist_points\", \"rolling_goal_points\", \"rolling_clean_sheet_points\", \"rolling_defensive_contribution_points\", \"rolling_penalty_miss_points\", \"rolling_goals_conceded_points\", \"rolling_yellow_card_points\", \"rolling_red_card_points\", \"rolling_own_goal_points\", \"avg_expected_goals\", \"avg_expected_assists\", \"avg_expected_goal_involvements\", \"avg_goals_scored\", \"avg_assists\", \"avg_total_points\", \"avg_minutes\", \"avg_clean_sheets\", \"avg_bps\", \"avg_ict_index\", \"avg_influence\", \"avg_creativity\", \"avg_threat\", \"avg_defensive_contribution\", \"avg_clearances_blocks_interceptions\", \"avg_bonus\", \"avg_saves\", \"avg_minutes_points\", \"avg_assist_points\", \"avg_goal_points\", \"avg_clean_sheet_points\", \"avg_defensive_contribution_points\", \"avg_penalty_miss_points\", \"avg_goals_conceded_points\", \"avg_yellow_card_points\", \"avg_red_card_points\", \"avg_own_goal_points\", \"rolling_points\", \"rolling_team_expected_goals\", \"rolling_expected_goals_against\", \"rolling_goal_difference\", \"avg_team_expected_goals\", \"avg_team_expected_assists\", \"avg_team_expected_goal_involvements\", \"avg_expected_goals_against\", \"avg_expected_assists_against\", \"avg_expected_goal_involvements_against\", \"avg_goal_difference\", \"match_points\", \"team_rolling_goals_conceded\", \"team_rolling_goal_difference\", \"player_share_of_team_xG\", \"player_share_of_team_points\", \"opponent_rolling_points\", \"opponent_rolling_team_expected_goals\", \"opponent_rolling_expected_goals_against\", \"opponent_rolling_goal_difference\", \"opponent_avg_team_expected_goals\", \"opponent_avg_team_expected_assists\", \"opponent_avg_team_expected_goal_involvements\", \"opponent_avg_expected_goals_against\", \"opponent_avg_expected_assists_against\", \"opponent_avg_expected_goal_involvements_against\", \"opponent_avg_goal_difference\"\n",
    "]\n",
    "\n",
    "# Fill missing columns with -1\n",
    "for col in feature_cols:\n",
    "    if col not in player_inference_features_df.columns:\n",
    "        player_inference_features_df = player_inference_features_df.withColumn(col, F.lit(-1))\n",
    "player_inference_features_df = player_inference_features_df.fillna(-1, subset=feature_cols)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "player_inference_df = assembler.transform(player_inference_features_df)\n",
    "\n",
    "# Load SparkML model from registry with UC volume path\n",
    "rf_model = mlflow.spark.load_model(model_uri, dfs_tmpdir=uc_volume_path)\n",
    "\n",
    "# Predict total_points\n",
    "predictions_df = rf_model.transform(player_inference_df)\n",
    "\n",
    "display(predictions_df)\n",
    "\n",
    "# Convert to Pandas and add prediction column\n",
    "inference_pdf = player_inference_df.toPandas()\n",
    "inference_pdf[\"predicted_total_points\"] = predictions_df.toPandas()[\"prediction\"]\n",
    "\n",
    "# Convert back to Spark DataFrame\n",
    "predicted_total_points_df = spark.createDataFrame(inference_pdf)\n",
    "\n",
    "display(predicted_total_points_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predict_next_gw_points",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}