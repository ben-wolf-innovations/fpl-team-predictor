{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3720f3-bde0-4cb9-b002-7063dccba12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ad945f-c4d7-4b3e-91fa-c7f10c0d97ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff544fcc-f81c-47a9-8016-9f93634d43e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b1424de-aa25-48f2-a877-86eb256cb6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENV = \"dev\"\n",
    "silver_schema = f\"fpl_silver_{ENV}\"\n",
    "feature_schema = f\"fpl_feature_{ENV}\"\n",
    "gold_schema = f\"fpl_gold_{ENV}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6b8fc6-b812-410f-bc1a-821f299c39e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim date"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "start_date = \"2016-08-01\"\n",
    "end_date = \"2029-08-01\"\n",
    "\n",
    "date_df = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) AS date_arr\"\n",
    "                     ).selectExpr(\"explode(date_arr) AS date\"\n",
    "                                  ).withColumns({\n",
    "        \"date_key\": F.date_format(\"date\", \"yyyyMMdd\").cast(\"int\"),\n",
    "        \"year\": F.year(\"date\"),\n",
    "        \"month\": F.month(\"date\"),\n",
    "        \"day\": F.dayofmonth(\"date\"),\n",
    "        \"day_name\": F.date_format(\"date\", \"EEEE\"),\n",
    "        \"day_name_short\": F.date_format(\"date\", \"E\"),\n",
    "        \"month_name\": F.date_format(\"date\", \"MMMM\"),\n",
    "        \"month_name_short\": F.date_format(\"date\", \"MMM\"),\n",
    "        \"month_year\": F.date_format(\"date\", \"yyyy-MM\"),\n",
    "        \"month_id\": F.date_format(\"date\", \"yyyyMM\").cast(\"int\"),\n",
    "        \"is_weekend\": F.when(F.date_format(\"date\", \"E\").isin(\"Sat\", \"Sun\"), F.lit(True)).otherwise(F.lit(False))\n",
    "    })\n",
    "\n",
    "merge_to_table(\n",
    "    df = date_df,\n",
    "    table_name = f\"{gold_schema}.dim_date\",\n",
    "    merge_condition = \"target.date_key = source.date_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0676cacd-37ee-45cd-9c45-6ee68586a14e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim players"
    }
   },
   "outputs": [],
   "source": [
    "#bi seasonal    \n",
    "\n",
    "#spell key to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100c48bf-f862-4735-955f-9060315fd08d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766085506089}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "dim teams"
    }
   },
   "outputs": [],
   "source": [
    "team_df = spark.table(f\"{silver_schema}.teams\")\n",
    "\n",
    "display(team_df)\n",
    "\n",
    "merge_to_table(\n",
    "    df = team_df,\n",
    "    table_name = f\"{gold_schema}.dim_team\",\n",
    "    merge_condition = \"target.team_season_key = source.team_season_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202033ae-81ba-47b1-8fc4-a6702a3bab60",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim season"
    }
   },
   "outputs": [],
   "source": [
    "season_df = spark.table(f\"{silver_schema}.seasons\")\n",
    "\n",
    "merge_to_table(\n",
    "    df = season_df,\n",
    "    table_name = f\"{gold_schema}.dim_season\",\n",
    "    merge_condition = \"target.season_key = source.season_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93adac7c-be6a-4868-838e-75f1ea06908d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim gameweek"
    }
   },
   "outputs": [],
   "source": [
    "gameweek_df = spark.table(f\"{silver_schema}.gameweeks\")\n",
    "\n",
    "merge_to_table(\n",
    "    df = gameweek_df,\n",
    "    table_name = f\"{gold_schema}.dim_gameweek\",\n",
    "    merge_condition = \"target.gameweek_key = source.gameweek_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec7a5cfc-9b90-43b2-acee-6dff6567a57c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim fixtures"
    }
   },
   "outputs": [],
   "source": [
    "#weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f2f787-f822-454d-adc6-f4bb8c827038",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "dim position"
    }
   },
   "outputs": [],
   "source": [
    "position_df = spark.table(f\"{silver_schema}.positions\")\n",
    "\n",
    "merge_to_table(\n",
    "    df = position_df,\n",
    "    table_name = f\"{gold_schema}.dim_position\",\n",
    "    merge_condition = \"target.position_key = source.position_key\",\n",
    "    spark = spark\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca95a3aa-d494-4614-aa50-54a6c403189e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact player stats"
    }
   },
   "outputs": [],
   "source": [
    "#weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76ca9f0a-ddbb-4485-8f91-4ea0c554c71e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact player fpl points"
    }
   },
   "outputs": [],
   "source": [
    "#need to add save points into gameweek stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8d829f4-4bbe-4636-8481-2c08a839b56f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact team gameweek stats"
    }
   },
   "outputs": [],
   "source": [
    "#roll up player gameweek stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d664fbf7-8de0-4497-a848-cb95ed45ae2e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact results"
    }
   },
   "outputs": [],
   "source": [
    "#similar to dim fixtures, has match stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f00e7005-f1bb-4a55-b2b5-76bde2f77711",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "fact predicted fpl vs actual"
    }
   },
   "outputs": [],
   "source": [
    "#merge predicted with fact player points (from silver)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dim_tables_season",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}