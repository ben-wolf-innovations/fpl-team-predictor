{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae49a207-e4b4-4057-ab86-e407107a229e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56aa4510-418a-4004-8dc2-0957310f1c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5634fa-c28c-4b3c-b8f3-af1e88009d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903ea271-4166-451f-8e41-eec4250185a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_to_df(\n",
    "    spark: SparkSession,\n",
    "    path: str,\n",
    "    header: bool = True,\n",
    "    infer_schema: bool = True,\n",
    "    schema: StructType = None,\n",
    "    delimiter: str = \",\",\n",
    "    encoding: str = \"UTF-8\",\n",
    "    quote: str = '\"',\n",
    "    escape: str = \"\\\\\",\n",
    "    null_value: str = None,\n",
    "    date_format: str = None,\n",
    "    timestamp_format: str = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generalised CSV reader for PySpark.\n",
    "\n",
    "    Parameters:\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - path (str): Path to the CSV file.\n",
    "    - header (bool): Whether the CSV has a header row.\n",
    "    - infer_schema (bool): Whether to infer schema automatically.\n",
    "    - schema (StructType, optional): Explicit schema to apply.\n",
    "    - delimiter (str): Field delimiter (default: ',').\n",
    "    - encoding (str): File encoding (default: 'UTF-8').\n",
    "    - quote (str): Quote character (default: '\"').\n",
    "    - escape (str): Escape character (default: '\\\\').\n",
    "    - null_value (str, optional): String to interpret as null.\n",
    "    - date_format (str, optional): Format for date columns.\n",
    "    - timestamp_format (str, optional): Format for timestamp columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Loaded Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    reader = spark.read.option(\"header\", str(header).lower()\n",
    "        ).option(\n",
    "            \"delimiter\", delimiter\n",
    "        ).option(\n",
    "            \"encoding\", encoding \n",
    "        ).option(\n",
    "            \"quote\", quote\n",
    "        ).option(\n",
    "            \"escape\", escape\n",
    "        )\n",
    "\n",
    "    if null_value:\n",
    "        reader = reader.option(\"nullValue\", null_value)\n",
    "    if date_format:\n",
    "        reader = reader.option(\"dateFormat\", date_format)\n",
    "    if timestamp_format:\n",
    "        reader = reader.option(\"timestampFormat\", timestamp_format)\n",
    "\n",
    "    if schema:\n",
    "        return reader.schema(schema).csv(path)\n",
    "    elif infer_schema:\n",
    "        return reader.option(\"inferSchema\", \"true\").csv(path)\n",
    "    else:\n",
    "        return reader.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ba3c29-5e2a-40c3-85fb-3f1465ae0a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENV = \"dev\"\n",
    "bronze_schema = f\"fpl_bronze_{ENV}\"\n",
    "\n",
    "base_path = \"/Volumes/workspace/fpl_raw/player_data\"\n",
    "output_base = f\"{bronze_schema}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d364ce27-7f6e-4811-8e30-3e1203f6aa78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to: fpl_bronze_dev.player_gameweek_stats_16_17\nWritten Delta table: player_gameweek_stats_16_17\nWriting to: fpl_bronze_dev.players_raw_16_17\nWritten Delta table: players_raw_16_17\nWriting to: fpl_bronze_dev.player_gameweek_stats_17_18\nWritten Delta table: player_gameweek_stats_17_18\nWriting to: fpl_bronze_dev.players_raw_17_18\nWritten Delta table: players_raw_17_18\nWriting to: fpl_bronze_dev.player_gameweek_stats_18_19\nWritten Delta table: player_gameweek_stats_18_19\nWriting to: fpl_bronze_dev.players_raw_18_19\nWritten Delta table: players_raw_18_19\nWriting to: fpl_bronze_dev.player_gameweek_stats_19_20\nWritten Delta table: player_gameweek_stats_19_20\nWriting to: fpl_bronze_dev.players_raw_19_20\nWritten Delta table: players_raw_19_20\nWriting to: fpl_bronze_dev.player_gameweek_stats_20_21\nWritten Delta table: player_gameweek_stats_20_21\nWriting to: fpl_bronze_dev.players_raw_20_21\nWritten Delta table: players_raw_20_21\nWriting to: fpl_bronze_dev.player_gameweek_stats_21_22\nWritten Delta table: player_gameweek_stats_21_22\nWriting to: fpl_bronze_dev.players_raw_21_22\nWritten Delta table: players_raw_21_22\nWriting to: fpl_bronze_dev.player_gameweek_stats_22_23\nWritten Delta table: player_gameweek_stats_22_23\nWriting to: fpl_bronze_dev.players_raw_22_23\nWritten Delta table: players_raw_22_23\nWriting to: fpl_bronze_dev.player_gameweek_stats_23_24\nWritten Delta table: player_gameweek_stats_23_24\nWriting to: fpl_bronze_dev.players_raw_23_24\nWritten Delta table: players_raw_23_24\nWriting to: fpl_bronze_dev.player_gameweek_stats_24_25\nWritten Delta table: player_gameweek_stats_24_25\nWriting to: fpl_bronze_dev.players_raw_24_25\nWritten Delta table: players_raw_24_25\nSkipping folder: 2025_26\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "    # Skip non-directories\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    # Extract season years\n",
    "    try:\n",
    "        start_year, end_year = folder.split(\"_\")\n",
    "        start_year_int = int(start_year)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping folder: {folder}\")\n",
    "        continue\n",
    "\n",
    "    # Skip folders from 2025_26 onwards here\n",
    "    if start_year_int >= 2025:\n",
    "        print(f\"Skipping folder: {folder}\")\n",
    "        continue\n",
    "\n",
    "    start_year_short = start_year[2:]\n",
    "\n",
    "    #ingest historic player gameweek stats\n",
    "    csv_file = f\"all_player_stats_{start_year_short}_{end_year}.csv\"\n",
    "    csv_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "    try:\n",
    "        df = read_csv_to_df(\n",
    "            spark=spark,\n",
    "            path=csv_path,\n",
    "            header=True,\n",
    "            infer_schema=True,\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    delta_table_name = f\"{output_base}.player_gameweek_stats_{start_year_short}_{end_year}\"\n",
    "\n",
    "    write_to_table(\n",
    "        df=df,\n",
    "        table_name=delta_table_name,\n",
    "        mode=\"overwrite\",\n",
    "        merge_schema=False  \n",
    "    )\n",
    "\n",
    "    print(f\"Written Delta table: player_gameweek_stats_{start_year_short}_{end_year}\")\n",
    "\n",
    "    #ingest historic players_raw data - detailed data per player\n",
    "    csv_file = f\"players_raw_{start_year_short}_{end_year}.csv\"\n",
    "    csv_path = os.path.join(folder_path, csv_file)\n",
    "\n",
    "    try:\n",
    "        df = read_csv_to_df(\n",
    "            spark=spark,\n",
    "            path=csv_path,\n",
    "            header=True,\n",
    "            infer_schema=True,\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    delta_table_name = f\"{output_base}.players_raw_{start_year_short}_{end_year}\"\n",
    "\n",
    "    write_to_table(\n",
    "        df=df,\n",
    "        table_name=delta_table_name,\n",
    "        mode=\"overwrite\",\n",
    "        merge_schema=False  \n",
    "    )\n",
    "\n",
    "    print(f\"Written Delta table: players_raw_{start_year_short}_{end_year}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_historic_player_stats",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}