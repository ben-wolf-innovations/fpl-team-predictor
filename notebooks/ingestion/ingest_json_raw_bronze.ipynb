{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbebcf2-9c7b-4bc9-8bcb-cee38b41374f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest JSON from Raw to Bronze\n",
    "\n",
    "Ingest bootstrap-static and fixtures from manually uploaded JSON files\n",
    "\n",
    "Write to bronze layer tables in fpl_bronze volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17dc47b-6b3e-4386-9b59-9e61d6967058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67777709-e8e0-4403-8941-5c0489e47919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ed4bd6-ec82-45b9-aa4f-00a6c4ad95ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_latest_raw_json(\n",
    "    base_path: str, \n",
    "    filename: str, \n",
    "    spark: SparkSession, \n",
    "    utils,\n",
    "    schema: StructType = None  # Optional schema\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the latest raw JSON file from a folder structure in DBFS.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Base path where folders are stored.\n",
    "    - filename (str): Name of the JSON file to read.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - utils: utility object (databricks or fabric).\n",
    "    - schema (StructType): Optional schema to apply during read.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Parsed Spark DataFrame from the latest folder.\n",
    "    \"\"\"\n",
    "    folders = utils.fs.ls(base_path)\n",
    "    latest_folder = sorted(folders, key=lambda x: x.name, reverse=True)[0].path\n",
    "    print(f\"Loading folder: {latest_folder}\")\n",
    "\n",
    "    read_options = spark.read.option(\"multiline\", \"true\")\n",
    "    if schema:\n",
    "        return read_options.schema(schema).json(f\"{latest_folder}/{filename}\")\n",
    "    else:\n",
    "        return read_options.json(f\"{latest_folder}/{filename}\")\n",
    "\n",
    "def ingest_entity(\n",
    "    entity_config: dict,\n",
    "    bronze_schema: str,\n",
    "    protocol: str,\n",
    "    season: str,\n",
    "    spark: SparkSession\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Ingests a single entity into the bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - entity_config (dict): Configuration for the entity.\n",
    "    - bronze_schema (str): Target schema name.\n",
    "    - protocol (str): Global ingestion protocol ('HIST' or 'INCR').\n",
    "    - season (str): current season suffix for tables\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "\n",
    "    The entity_config must include:\n",
    "    - name (str): Entity name.\n",
    "    - df (DataFrame): Source DataFrame.\n",
    "    - protocol (str): Entity-specific protocol.\n",
    "    Optional keys:\n",
    "    - path (str): Column to select or explode.\n",
    "    - explode (bool): Whether to explode the path.\n",
    "    - alias (str): Alias for exploded column.\n",
    "    - merge_key (str): Key to use for merge condition.\n",
    "    \"\"\"\n",
    "    name = entity_config[\"name\"] + \"_\" + season\n",
    "    df = entity_config[\"df\"]\n",
    "    path = entity_config.get(\"path\")\n",
    "    explode = entity_config.get(\"explode\", False)\n",
    "    alias = entity_config.get(\"alias\")\n",
    "    merge_key = entity_config.get(\"merge_key\")\n",
    "    entity_protocol = entity_config[\"protocol\"]\n",
    "\n",
    "    # Extract and transform\n",
    "    if explode and path:\n",
    "        entity_df = df.select(F.explode(path).alias(alias)).select(f\"{alias}.*\")\n",
    "    elif path:\n",
    "        entity_df = df.select(path)\n",
    "    else:\n",
    "        entity_df = df\n",
    "\n",
    "    # # Detect schema drift\n",
    "    # detect_schema_drift(\n",
    "    #     new_df=entity_df,\n",
    "    #     table_name=f\"{bronze_schema}.{name}\",\n",
    "    #     spark=spark\n",
    "    # )\n",
    "    \n",
    "    # Write or merge\n",
    "    if protocol == \"HIST\":\n",
    "        write_to_table(\n",
    "            df=entity_df,\n",
    "            table_name=f\"{bronze_schema}.{name}\"\n",
    "        )\n",
    "        print(f\"[HIST] {name} written to {bronze_schema}.{name}.\")\n",
    "    elif entity_protocol == \"INCR\" and protocol == \"INCR\":\n",
    "        merge_to_table(\n",
    "            df=entity_df,\n",
    "            table_name=f\"{bronze_schema}.{name}\",\n",
    "            merge_condition=f\"target.{merge_key} = source.{merge_key}\",\n",
    "            spark=spark\n",
    "        )\n",
    "        print(f\"[INCR] {name} merged to {bronze_schema}.{name}.\")\n",
    "\n",
    "def normalize_fixtures_schema(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Normalizes the FPL fixtures 'stats' column so that every row conforms to\n",
    "    Array<Struct<identifier: string, a: Array<Struct<element: bigint, value: bigint>>, h: Array<Struct<element: bigint, value: bigint>>>>.\n",
    "    Works when some rows have [] or null and others have full structs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define canonical schema for `stats`\n",
    "    stats_struct_type = T.ArrayType(\n",
    "        T.StructType([\n",
    "            T.StructField(\"a\", T.ArrayType(T.StructType([\n",
    "                T.StructField(\"element\", T.LongType(), True),\n",
    "                T.StructField(\"value\", T.LongType(), True)\n",
    "            ])), True),\n",
    "            T.StructField(\"h\", T.ArrayType(T.StructType([\n",
    "                T.StructField(\"element\", T.LongType(), True),\n",
    "                T.StructField(\"value\", T.LongType(), True)\n",
    "            ])), True),\n",
    "            T.StructField(\"identifier\", T.StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    # Create an empty array of the correct type\n",
    "    empty_array = F.lit([]).cast(stats_struct_type)\n",
    "\n",
    "    # Normalize the stats column\n",
    "    df = df.withColumn(\n",
    "        \"stats\",\n",
    "        F.when(\n",
    "            (F.col(\"stats\").isNull()) | (F.size(F.col(\"stats\")) == 0),\n",
    "            empty_array\n",
    "        ).otherwise(F.col(\"stats\").cast(stats_struct_type))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14353721-dd2b-495e-a211-13d6126cdb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")\n",
    "\n",
    "def detect_schema_drift(new_df: DataFrame, table_name: str, spark: SparkSession) -> bool:\n",
    "    \"\"\"\n",
    "    Detects schema drift between a new DataFrame and an existing Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - new_df (DataFrame): The new DataFrame to compare.\n",
    "    - table_name (str): The name of the existing Delta table.\n",
    "    - spark (SparkSession): The active Spark session.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if schema drift is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        existing_df = spark.table(table_name)\n",
    "        existing_fields = set(field.name for field in existing_df.schema.fields if field.name != \"last_updated\")\n",
    "        new_fields = set(field.name for field in new_df.schema.fields if field.name != \"last_updated\")\n",
    "\n",
    "        added = new_fields - existing_fields\n",
    "        removed = existing_fields - new_fields\n",
    "\n",
    "        if added or removed:\n",
    "            print(f\"Schema drift detected in {table_name}\")\n",
    "            if added:\n",
    "                print(f\"Added fields: {added}\")\n",
    "            if removed:\n",
    "                print(f\"Removed fields: {removed}\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        print(f\"ℹ No existing table found for {table_name}. Assuming first write.\")\n",
    "        return False\n",
    "\n",
    "def merge_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    merge_condition: str,\n",
    "    spark: SparkSession,\n",
    "    partition_by: list[str] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Performs an upsert (merge) into a Delta table.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Incoming DataFrame to merge.\n",
    "    - table_name (str): Target Delta table name.\n",
    "    - merge_condition (str): SQL condition for matching rows.\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - partition_by (list[str], optional): Columns to partition by on initial write.\n",
    "\n",
    "    If the table does not exist, it will be created using write_to_table.\n",
    "    \"\"\"\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        write_to_table(\n",
    "            df=df_with_ts,\n",
    "            table_name=table_name,\n",
    "            partition_by=partition_by\n",
    "        )\n",
    "    else:\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "        (\n",
    "            delta_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source=df_with_ts.alias(\"source\"),\n",
    "                condition=merge_condition\n",
    "            )\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7cb87a2-286d-4119-9ed1-a210508e5009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de1a84d-bb48-4485-8463-286674cf869a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    ENV = dbutils.widgets.get(\"ENV\")\n",
    "except Exception:\n",
    "    ENV = \"dev\"\n",
    "\n",
    "try:\n",
    "    PROTOCOL = dbutils.widgets.get(\"PROTOCOL\")\n",
    "except Exception:\n",
    "    PROTOCOL = \"HIST\"\n",
    "\n",
    "#ensure valid ENV and PROTOCOL\n",
    "valid_envs = {\"dev\", \"test\", \"prod\"}\n",
    "valid_protocols = {\"HIST\", \"INCR\"}\n",
    "\n",
    "# Validate ENV\n",
    "if ENV not in valid_envs:\n",
    "    print(f\"Invalid ENV: {ENV}. Must be one of {valid_envs}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid ENV\")\n",
    "\n",
    "# Validate PROTOCOL\n",
    "if PROTOCOL not in valid_protocols:\n",
    "    print(f\"Invalid PROTOCOL: {PROTOCOL}. Must be one of {valid_protocols}. Exiting notebook.\")\n",
    "    dbutils.notebook.exit(\"Invalid PROTOCOL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1decc49-8dd3-48f4-a23b-189200475099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BRONZE_SCHEMA = f\"fpl_bronze_{ENV}\"\n",
    "CURRENT_SEASON = \"2025_26\"\n",
    "CURRENT_SEASON_SHORT = CURRENT_SEASON[2:]\n",
    "BASE_RAW_JSON_PATH = f\"/Volumes/workspace/fpl_raw/player_data/{CURRENT_SEASON}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0c9e511-e8bc-4767-8268-a50ced5a46b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest Raw JSON files\n",
    "\n",
    "bootstrap-static is core data, with schema as:\n",
    "\n",
    "- events: Basic information of every Gameweek such as average score, highest score, top scoring player, most captained, etc. Incremental\n",
    "- game_settings: The game settings and rules. \n",
    "- phases: Phases of FPL season. \n",
    "- teams: Basic information of current Premier League clubs.\n",
    "- total_players: Total FPL players.\n",
    "- elements: Information of all Premier League players including points, status, value, match stats (goals, assists, etc.), ICT index, etc. Incremental\n",
    "- element_types: Basic information about player’s position (GK, DEF, MID, FWD).\n",
    "- chips: All chips available in FPL.\n",
    "- game_config: scoring and game setup rules.\n",
    "\n",
    "\n",
    "fixtures contains all data about fixtures for the season. It needs to be incrementally loaded as fixtures change often due to clashes/TV viewing changes.\n",
    "\n",
    "Ingested on a weekly basis (after end of gameweek) and tables written to {table_name}_{season} e.g. events_25_26 for future season ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9966e07-5ec8-4959-9a69-e205c6073421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folder: dbfs:/Volumes/workspace/fpl_raw/player_data/2025_26/gw_08/\nLoading folder: dbfs:/Volumes/workspace/fpl_raw/player_data/2025_26/gw_08/\n"
     ]
    }
   ],
   "source": [
    "bootstrap_static_df = read_latest_raw_json(\n",
    "    base_path = BASE_RAW_JSON_PATH, \n",
    "    filename = \"bootstrap_static.json\",\n",
    "    spark = spark,\n",
    "    utils = dbutils\n",
    "    )\n",
    "    \n",
    "fixtures_df = read_latest_raw_json(\n",
    "    base_path = BASE_RAW_JSON_PATH, \n",
    "    filename = \"fixtures.json\",\n",
    "    spark = spark,\n",
    "    utils = dbutils\n",
    "    )\n",
    "\n",
    "fixtures_df = normalize_fixtures_schema(fixtures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1255c042-bf9b-4458-a74f-4ec55f38e50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- code: long (nullable = true)\n |-- event: long (nullable = true)\n |-- finished: boolean (nullable = true)\n |-- finished_provisional: boolean (nullable = true)\n |-- id: long (nullable = true)\n |-- kickoff_time: string (nullable = true)\n |-- minutes: long (nullable = true)\n |-- provisional_start_time: boolean (nullable = true)\n |-- pulse_id: long (nullable = true)\n |-- started: boolean (nullable = true)\n |-- stats: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- a: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- element: long (nullable = true)\n |    |    |    |    |-- value: long (nullable = true)\n |    |    |-- h: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- element: long (nullable = true)\n |    |    |    |    |-- value: long (nullable = true)\n |    |    |-- identifier: string (nullable = true)\n |-- team_a: long (nullable = true)\n |-- team_a_difficulty: long (nullable = true)\n |-- team_a_score: long (nullable = true)\n |-- team_h: long (nullable = true)\n |-- team_h_difficulty: long (nullable = true)\n |-- team_h_score: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "fixtures_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29909812-4bff-4b2d-96f5-faed9f700a75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENTITY_CONFIG = [\n",
    "    {\n",
    "        \"name\": \"chips\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"chips\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"chip\",\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"element_stats\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"element_stats\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"stat\",\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"element_types\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"element_types\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"type\",\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"game_config_scoring\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"game_config.scoring\",\n",
    "        \"season\": CURRENT_SEASON_SHORT,\n",
    "        \"explode\": False,\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"game_config_rules\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"game_config.rules\",\n",
    "        \"explode\": False,\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"phases\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"phases\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"phase\",\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"teams\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"teams\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"team\",\n",
    "        \"protocol\": \"HIST\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"elements\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"elements\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"player\",\n",
    "        \"protocol\": \"INCR\",\n",
    "        \"merge_key\": \"id\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"events\",\n",
    "        \"df\": bootstrap_static_df,\n",
    "        \"path\": \"events\",\n",
    "        \"explode\": True,\n",
    "        \"alias\": \"event\",\n",
    "        \"protocol\": \"INCR\",\n",
    "        \"merge_key\": \"id\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"fixtures\",\n",
    "        \"df\": fixtures_df,\n",
    "        \"path\": None,\n",
    "        \"explode\": False,\n",
    "        \"protocol\": \"INCR\",\n",
    "        \"merge_key\": \"id\"\n",
    "    }\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc43edac-6584-46f8-af79-168806e01bf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INCR] elements_25_26 merged to fpl_bronze_dev.elements_25_26.\n[INCR] events_25_26 merged to fpl_bronze_dev.events_25_26.\n[INCR] fixtures_25_26 merged to fpl_bronze_dev.fixtures_25_26.\n"
     ]
    }
   ],
   "source": [
    "for entity in ENTITY_CONFIG:\n",
    "    ingest_entity(\n",
    "        entity_config = entity,\n",
    "        bronze_schema = BRONZE_SCHEMA,\n",
    "        protocol = PROTOCOL,\n",
    "        season = CURRENT_SEASON_SHORT,\n",
    "        spark = spark\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca17659c-6b08-4cfa-abc5-3e7917000f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Player Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f61b08-8e91-4675-8a82-7d9a8b7cf48c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_json_files(base_path: str, pattern: str) -> list:\n",
    "    \"\"\"\n",
    "    Recursively list all JSON files under base_path that contain the pattern in their filename.\n",
    "    \"\"\"\n",
    "    all_files = []\n",
    "\n",
    "    def recurse(path):\n",
    "        items = dbutils.fs.ls(path)\n",
    "        for item in items:\n",
    "            if item.isDir():\n",
    "                recurse(item.path)\n",
    "            elif item.path.endswith(\".json\") and pattern in item.path:\n",
    "                all_files.append(item.path)\n",
    "\n",
    "    recurse(base_path)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc1ec835-395b-45a2-ae82-5add4a9c2a2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for each history element\n",
    "history_schema_25_26 = ArrayType(StructType([\n",
    "    StructField(\"assists\", IntegerType()),\n",
    "    StructField(\"bonus\", IntegerType()),\n",
    "    StructField(\"bps\", IntegerType()),\n",
    "    StructField(\"clean_sheets\", IntegerType()),\n",
    "    StructField(\"clearances_blocks_interceptions\", IntegerType()),\n",
    "    StructField(\"creativity\", StringType()),\n",
    "    StructField(\"defensive_contribution\", IntegerType()),\n",
    "    StructField(\"element\", IntegerType()),\n",
    "    StructField(\"expected_assists\", StringType()),\n",
    "    StructField(\"expected_goal_involvements\", StringType()),\n",
    "    StructField(\"expected_goals\", StringType()),\n",
    "    StructField(\"expected_goals_conceded\", StringType()),\n",
    "    StructField(\"fixture\", IntegerType()),\n",
    "    StructField(\"goals_conceded\", IntegerType()),\n",
    "    StructField(\"goals_scored\", IntegerType()),\n",
    "    StructField(\"ict_index\", StringType()),\n",
    "    StructField(\"influence\", StringType()),\n",
    "    StructField(\"kickoff_time\", StringType()),\n",
    "    StructField(\"minutes\", IntegerType()),\n",
    "    StructField(\"modified\", BooleanType()),\n",
    "    StructField(\"opponent_team\", IntegerType()),\n",
    "    StructField(\"own_goals\", IntegerType()),\n",
    "    StructField(\"penalties_missed\", IntegerType()),\n",
    "    StructField(\"penalties_saved\", IntegerType()),\n",
    "    StructField(\"recoveries\", IntegerType()),\n",
    "    StructField(\"red_cards\", IntegerType()),\n",
    "    StructField(\"round\", IntegerType()),\n",
    "    StructField(\"saves\", IntegerType()),\n",
    "    StructField(\"selected\", IntegerType()),\n",
    "    StructField(\"starts\", IntegerType()),\n",
    "    StructField(\"tackles\", IntegerType()),\n",
    "    StructField(\"team_a_score\", IntegerType()),\n",
    "    StructField(\"team_h_score\", IntegerType()),\n",
    "    StructField(\"threat\", StringType()),\n",
    "    StructField(\"total_points\", IntegerType()),\n",
    "    StructField(\"transfers_balance\", IntegerType()),\n",
    "    StructField(\"transfers_in\", IntegerType()),\n",
    "    StructField(\"transfers_out\", IntegerType()),\n",
    "    StructField(\"value\", IntegerType()),\n",
    "    StructField(\"was_home\", BooleanType()),\n",
    "    StructField(\"yellow_cards\", IntegerType())\n",
    "]))\n",
    "\n",
    "# Full schema includes player id and history array\n",
    "full_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"history\", history_schema_25_26)\n",
    "])\n",
    "\n",
    "player_stats_table = f\"{BRONZE_SCHEMA}.player_gameweek_stats_{CURRENT_SEASON_SHORT}\"\n",
    "\n",
    "PROTOCOL = \"HIST\"\n",
    "\n",
    "if PROTOCOL == \"HIST\":\n",
    "    # Read all player_stats JSON files across all gameweeks\n",
    "    json_files = list_all_json_files(\n",
    "        base_path = BASE_RAW_JSON_PATH,\n",
    "        pattern = \"player_stats\")\n",
    "    \n",
    "    if not json_files:\n",
    "        raise ValueError(\"No player_stats JSON files found for HIST protocol.\")\n",
    "\n",
    "    player_stats_df = None\n",
    "    # Read JSON with schema\n",
    "    for path in json_files:\n",
    "        df = spark.read.option(\"multiline\", \"true\").schema(full_schema).json(path) \n",
    "\n",
    "        exploded_df = df.select(\n",
    "                F.col(\"id\").alias(\"player_id\"),\n",
    "                F.explode(\"history\").alias(\"stats\")\n",
    "            ).select(\n",
    "                F.col(\"player_id\"),\n",
    "                F.col(\"stats.*\")\n",
    "            )\n",
    "\n",
    "        player_stats_df = exploded_df if player_stats_df is None else player_stats_df.unionByName(exploded_df)\n",
    "\n",
    "    write_to_table(\n",
    "        df = player_stats_df,\n",
    "        table_name = player_stats_table\n",
    "    )\n",
    "\n",
    "elif PROTOCOL == \"INCR\":\n",
    "    # Use your existing function to read the latest JSON\n",
    "    latest_df = read_latest_raw_json(\n",
    "        base_path=BASE_RAW_JSON_PATH,\n",
    "        filename=f\"player_stats_{CURRENT_SEASON}_*.json\",\n",
    "        spark=spark,\n",
    "        utils=dbutils,\n",
    "        schema = full_schema\n",
    "    )\n",
    "\n",
    "    exploded_df = latest_df.select(\n",
    "        F.col(\"id\").alias(\"player_id\"),\n",
    "        F.explode(\"history\").alias(\"stats\")\n",
    "    ).select(\n",
    "        F.col(\"player_id\"),\n",
    "        F.col(\"stats.*\")\n",
    "    )\n",
    "\n",
    "    merge_to_table(\n",
    "        df=exploded_df,\n",
    "        table_name= player_stats_table,\n",
    "        merge_condition=\"target.player_id= source.player_id AND target.round = source.round\",\n",
    "        spark=spark\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35d01bfe-dcf6-43ee-9269-96107d7c4810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Step 1: Read the file with schema\n",
    "df = spark.read.option(\"multiline\", \"true\").schema(full_schema).json(\n",
    "    \"/Volumes/workspace/fpl_raw/player_data/2025_26/gw_07/all_player_stats_25_26_gw07.json\"\n",
    ")\n",
    "\n",
    "# Step 2: Explode history\n",
    "exploded_df = df.select(\n",
    "    F.col(\"id\").alias(\"player_id\"),\n",
    "    F.explode(\"history\").alias(\"stats\")\n",
    ")\n",
    "\n",
    "# Step 3: Filter out round 8\n",
    "filtered_df = exploded_df.filter(F.col(\"stats.round\") != 8)\n",
    "\n",
    "# Step 4: Rebuild history array\n",
    "reassembled_df = filtered_df.groupBy(\"player_id\").agg(\n",
    "    F.collect_list(\"stats\").alias(\"history\")\n",
    ").select(\n",
    "    F.col(\"player_id\").alias(\"id\"),\n",
    "    F.col(\"history\")\n",
    ")\n",
    "\n",
    "# Step 5: Write back to JSON\n",
    "reassembled_df.write.mode(\"overwrite\").json(\n",
    "    \"/Volumes/workspace/fpl_raw/player_data/2025_26/gw_07/all_player_stats_25_26_gw07_cleaned.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "337d5ddf-e5f5-436c-a501-cb11d2939cff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "472d272e-2bfa-4b61-8d69-c84d070bdbbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84b48c57-90e8-416f-a196-399d6e8b96a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_json_raw_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}