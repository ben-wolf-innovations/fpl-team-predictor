{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3af3fa-ba59-4a9e-90a8-1b348a485878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8cc4d2-97b4-4ee5-8402-810139733c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a6cc9f-efd1-4f32-863f-1e499177d983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_table(\n",
    "    df: DataFrame,\n",
    "    table_name: str,\n",
    "    mode: str = \"overwrite\",\n",
    "    merge_schema: bool = True,\n",
    "    partition_by: list[str] = None,\n",
    "    path: str = None,\n",
    "    save_as_table: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generalised Delta write helper for bronze layer.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Spark DataFrame to write.\n",
    "    - table_name (str): Name of the Delta table (used if save_as_table=True).\n",
    "    - mode (str): Write mode ('overwrite', 'append', 'ignore', 'error', etc.).\n",
    "    - merge_schema (bool): Whether to merge schema on write.\n",
    "    - partition_by (list[str], optional): List of columns to partition by.\n",
    "    - path (str, optional): Path to save the Delta table (used if save_as_table=False).\n",
    "    - save_as_table (bool): If True, saves as managed table; else saves to path.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If neither save_as_table nor path is properly specified.\n",
    "    \"\"\"\n",
    "\n",
    "    df_with_ts = df.withColumn(\"last_updated\", F.current_timestamp())\n",
    "\n",
    "    writer = df_with_ts.write.format(\"delta\").mode(mode)\n",
    "\n",
    "    if merge_schema:\n",
    "        writer = writer.option(\"mergeSchema\", \"true\")\n",
    "    elif mode == \"overwrite\":\n",
    "        writer = writer.option(\"overwriteSchema\", \"true\")\n",
    "\n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(*partition_by)\n",
    "\n",
    "    if save_as_table:\n",
    "        writer.saveAsTable(table_name)\n",
    "    elif path:\n",
    "        writer.save(path)\n",
    "    else:\n",
    "        raise ValueError(\"Either save_as_table must be True or a path must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "579712e1-f94c-4d6b-b031-af182da87230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_to_df(\n",
    "    spark: SparkSession,\n",
    "    path: str,\n",
    "    header: bool = True,\n",
    "    infer_schema: bool = True,\n",
    "    schema: StructType = None,\n",
    "    delimiter: str = \",\",\n",
    "    encoding: str = \"UTF-8\",\n",
    "    quote: str = '\"',\n",
    "    escape: str = \"\\\\\",\n",
    "    null_value: str = None,\n",
    "    date_format: str = None,\n",
    "    timestamp_format: str = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generalised CSV reader for PySpark.\n",
    "\n",
    "    Parameters:\n",
    "    - spark (SparkSession): Active Spark session.\n",
    "    - path (str): Path to the CSV file.\n",
    "    - header (bool): Whether the CSV has a header row.\n",
    "    - infer_schema (bool): Whether to infer schema automatically.\n",
    "    - schema (StructType, optional): Explicit schema to apply.\n",
    "    - delimiter (str): Field delimiter (default: ',').\n",
    "    - encoding (str): File encoding (default: 'UTF-8').\n",
    "    - quote (str): Quote character (default: '\"').\n",
    "    - escape (str): Escape character (default: '\\\\').\n",
    "    - null_value (str, optional): String to interpret as null.\n",
    "    - date_format (str, optional): Format for date columns.\n",
    "    - timestamp_format (str, optional): Format for timestamp columns.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Loaded Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    reader = spark.read.option(\"header\", str(header).lower()\n",
    "        ).option(\n",
    "            \"delimiter\", delimiter\n",
    "        ).option(\n",
    "            \"encoding\", encoding \n",
    "        ).option(\n",
    "            \"quote\", quote\n",
    "        ).option(\n",
    "            \"escape\", escape\n",
    "        )\n",
    "\n",
    "    if null_value:\n",
    "        reader = reader.option(\"nullValue\", null_value)\n",
    "    if date_format:\n",
    "        reader = reader.option(\"dateFormat\", date_format)\n",
    "    if timestamp_format:\n",
    "        reader = reader.option(\"timestampFormat\", timestamp_format)\n",
    "\n",
    "    if schema:\n",
    "        return reader.schema(schema).csv(path)\n",
    "    elif infer_schema:\n",
    "        return reader.option(\"inferSchema\", \"true\").csv(path)\n",
    "    else:\n",
    "        return reader.csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6459b527-b64d-4ec5-ac2d-22862992ee6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teams_csv_path = \"/Volumes/workspace/fpl_raw/player_data/fpl_teams.csv\"\n",
    "BRONZE_SCHEMA = \"fpl_bronze_dev\"\n",
    "SILVER_SCHEMA = \"fpl_silver_dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60803ffa-1713-4aa3-aacc-1bc9bccc8b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teams_df = read_csv_to_df(\n",
    "    spark=spark,\n",
    "    path=teams_csv_path,\n",
    "    header=True,\n",
    "    infer_schema=True\n",
    ")\n",
    "\n",
    "#write raw to bronze\n",
    "write_to_table(\n",
    "    df = teams_df,\n",
    "    table_name = f\"{BRONZE_SCHEMA}.teams\",\n",
    "    mode = \"overwrite\",\n",
    "    merge_schema = False\n",
    ")\n",
    "\n",
    "#cast columns to expected types and write to silver\n",
    "silver_teams_df = teams_df.select(\n",
    "    F.col(\"season_key\").cast(\"int\").alias(\"season_key\"),\n",
    "    F.col(\"team_code\").cast(\"int\"),\n",
    "    F.col(\"team_id\").cast(\"int\"),\n",
    "    F.col(\"team_name\").cast(\"string\"),\n",
    "    F.col(\"team_name_short\").cast(\"string\"),\n",
    "    F.col(\"is_promoted\").cast(\"boolean\"),\n",
    "    F.col(\"is_relegated\").cast(\"boolean\")\n",
    ")\n",
    "\n",
    "write_to_table(\n",
    "    df = silver_teams_df,\n",
    "    table_name = f\"{SILVER_SCHEMA}.teams\",\n",
    "    mode = \"overwrite\",\n",
    "    merge_schema = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_teams",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}