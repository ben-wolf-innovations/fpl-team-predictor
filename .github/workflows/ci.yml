name: CI Pipeline

on:
  push:
    branches:
      - feature/*
      - test
      - dev
      - prod
  pull_request:
    branches:
      - feature/*
      - test
      - dev
      - prod

jobs:
  test:
    runs-on: ubuntu-latest

    env:
      PYSPARK_SUBMIT_ARGS: >
        --packages io.delta:delta-core_2.13:2.4.0
        --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        pyspark-shell

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Java
      run: |
        sudo apt-get update
        sudo apt-get install openjdk-11-jdk -y
        java -version

    - name: Set JAVA_HOME
      run: |
        echo "JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))" >> $GITHUB_ENV

    - name: Install Python dependencies
      run: |
        pip install -r requirements.txt
        # pin pyspark so Spark/Scala version is known and matches delta-core artifact
        pip install pyspark==3.5.2 chispa pytest

    - name: Debug Spark/Python versions
      run: |
        python - <<'PY'
        import pyspark
        print("pyspark", pyspark.__version__)
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.master("local[1]").getOrCreate()
        print("spark.version", spark.version)
        spark.stop()
        PY

    - name: Run unit tests
      run: |
        export PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.13:2.4.0 \
          --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
          --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell"
        export PYTHONPATH=$(pwd)/src
        python -m pytest tests/